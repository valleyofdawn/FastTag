{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valleyofdawn/FastTag/blob/main/FastTag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "084b7487-0271-4014-9132-17ff1365e4b0",
      "metadata": {
        "id": "084b7487-0271-4014-9132-17ff1365e4b0"
      },
      "source": [
        "# Single cell annotation pipline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b81b97-c2c0-4581-a773-de9830a0096a",
      "metadata": {
        "id": "98b81b97-c2c0-4581-a773-de9830a0096a"
      },
      "source": [
        "## Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hZUZHOog3Rwj",
      "metadata": {
        "id": "hZUZHOog3Rwj"
      },
      "source": [
        "### Install missing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "CVc_rnjr3PIl",
      "metadata": {
        "id": "CVc_rnjr3PIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9979faa6-ccf2-407c-c8ae-7d9d681e5ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/169.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m255.4/255.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-storage scanpy anndata mygene harmonypy igraph leidenalg scikit-misc --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PJ9DfbHsZXFs",
      "metadata": {
        "id": "PJ9DfbHsZXFs"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "933365cc-e9cd-410a-a2a6-656094c480e8",
      "metadata": {
        "id": "933365cc-e9cd-410a-a2a6-656094c480e8"
      },
      "source": [
        "### load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f0509582-4f82-41b1-9312-2761e2666a1c",
      "metadata": {
        "id": "f0509582-4f82-41b1-9312-2761e2666a1c"
      },
      "outputs": [],
      "source": [
        "from anndata import AnnData # for rading 10x format\n",
        "from google.cloud import storage\n",
        "from google.colab import auth, drive\n",
        "from pathlib import Path # needed for multiple file input\n",
        "from scipy import sparse as sp\n",
        "from scipy.io import mmread\n",
        "from scipy.sparse import csr_matrix # for rading 10x format\n",
        "from scipy.stats import pearsonr, ttest_ind, ttest_rel # for leave-one-out marker pruning\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.preprocessing import StandardScaler, minmax_scale\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from typing import Literal, Sequence, Tuple, Optional\n",
        "\n",
        "import anndata as ad # needed throughout\n",
        "import gc, glob, gzip, h5py, harmonypy as hm, igraph as ig, io, matplotlib.pyplot as plt\n",
        "import mygene as mg, numpy as np, openai, os, pandas as pd, re, requests, scanpy as sc, scipy.sparse as sp, seaborn as sns\n",
        "import tarfile, textwrap, time, yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc071a2-ac45-41ff-a1b8-30e8e3939c69",
      "metadata": {
        "id": "bdc071a2-ac45-41ff-a1b8-30e8e3939c69"
      },
      "source": [
        "### Set variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c44abe5a-4d92-45ea-91ca-9f7a977cfaf2",
      "metadata": {
        "id": "c44abe5a-4d92-45ea-91ca-9f7a977cfaf2"
      },
      "outputs": [],
      "source": [
        "sc.settings.verbosity = 0             # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
        "sc.logging.print_header()\n",
        "set4 =  ['darkgreen', 'cornflowerblue', 'darkred', 'plum', 'orange',\n",
        "    'magenta', 'lightgreen', 'tan', 'slateblue', 'gray', 'turquoise',\n",
        "    'coral', 'lightgray', 'green', 'pink', 'gold', 'peru',\n",
        "    'red', 'olive', 'lime', 'teal', 'skyblue', 'violet',\n",
        "    'crimson', 'beige', 'lightgrey',  'thistle','darkorange',\n",
        "    'lightgreen', 'blue', 'wheat','orchid']\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (6, 6)\n",
        "os.environ['LOKY_MAX_CPU_COUNT'] = '12'\n",
        "pd.set_option('display.float_format', '{:.2e}'.format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WNPLS8_tqvSs",
      "metadata": {
        "id": "WNPLS8_tqvSs"
      },
      "source": [
        "### Connect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9KrVjXQkSXaj",
      "metadata": {
        "id": "9KrVjXQkSXaj"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('openai_client')\n",
        "# os.chdir('/content/')\n",
        "# print('Current working directory:', os.getcwd())\n",
        "auth.authenticate_user()\n",
        "client = storage.Client(project='Scanpy for Biolojic')\n",
        "bucket = client.bucket('single_cell_h5ad_datasets')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceae61aa-b53c-42be-9f34-7c20dd5ec778",
      "metadata": {
        "id": "ceae61aa-b53c-42be-9f34-7c20dd5ec778"
      },
      "source": [
        "### Function definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2mHfEqT5t2q",
      "metadata": {
        "id": "b2mHfEqT5t2q"
      },
      "source": [
        "#### Make Pseudobulk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "byYt81dKm7dI",
      "metadata": {
        "id": "byYt81dKm7dI"
      },
      "outputs": [],
      "source": [
        "def generate_pseudobulk_matrix(\n",
        "    adata, groupby_cols: list = ['patient', 'timepoint']\n",
        "):\n",
        "    obs = adata.obs[groupby_cols].copy()\n",
        "\n",
        "    # Combine grouping columns into a single label\n",
        "    group_labels = obs.astype(str).agg('_'.join, axis=1)\n",
        "    unique_labels = group_labels.unique()\n",
        "\n",
        "    # Access expression matrix\n",
        "    X = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
        "    df = pd.DataFrame(X, index=group_labels, columns=adata.var_names)\n",
        "\n",
        "    # Compute mean expression per group\n",
        "    pseudobulk = df.groupby(level=0).mean()\n",
        "\n",
        "    # Recover metadata from group_labels\n",
        "    metadata = pd.DataFrame(\n",
        "        [label.split('_') for label in pseudobulk.index],\n",
        "        columns=groupby_cols,\n",
        "        index=pseudobulk.index\n",
        "    )\n",
        "\n",
        "    # Return metadata + expression matrix\n",
        "    return pd.concat([metadata, pseudobulk], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "njYwZfNp13EL",
      "metadata": {
        "id": "njYwZfNp13EL"
      },
      "source": [
        "#### Pseudobulk T-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "nGIoN8Vb1z6K",
      "metadata": {
        "id": "nGIoN8Vb1z6K"
      },
      "outputs": [],
      "source": [
        "def ttest_pseudobulk(\n",
        "    pb_df,\n",
        "    gene: str,\n",
        "    condition_col: str = \"treatment\",\n",
        "    subject_col: str = \"patient\", # Modified: Default remains 'patient'\n",
        "    paired: bool = True, # Modified: Default remains True\n",
        "    alternative: str = \"both\",  # 'both', 'pos', 'neg'\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform a t-test on pseudobulk data for a given gene.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pb_df : pd.DataFrame\n",
        "        Pseudobulk matrix with subjects, conditions, and gene expression.\n",
        "    gene : str\n",
        "        Gene name (must be a column in pb_df).\n",
        "    condition_col : str\n",
        "        Column name indicating treatment condition.\n",
        "    subject_col : str or None\n",
        "        Column name identifying subjects (for pairing). If None, performs unpaired test.\n",
        "    paired : bool\n",
        "        Whether to use a paired t-test. Ignored if subject_col is None.\n",
        "    alternative : str\n",
        "        'both' = two-sided, 'pos' = greater (treated > control), 'neg' = less.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    t_stat : float\n",
        "    p_value : float\n",
        "    summary : str\n",
        "        Text summary of test result.\n",
        "    \"\"\"\n",
        "    df = pb_df[[col for col in [subject_col, condition_col, gene] if col is not None]].dropna() # Modified: Select columns only if they are not None\n",
        "    conditions = df[condition_col].unique()\n",
        "\n",
        "    if len(conditions) != 2:\n",
        "        raise ValueError(\"T-test requires exactly two conditions.\")\n",
        "\n",
        "    cond1, cond2 = sorted(conditions)  # ensure consistent order\n",
        "\n",
        "    # Modified: Check if subject_col is None or paired is False\n",
        "    if subject_col is None or not paired:\n",
        "        print(\"Performing unpaired t-test.\")\n",
        "        x1 = df[df[condition_col] == cond1][gene].values\n",
        "        x2 = df[df[condition_col] == cond2][gene].values\n",
        "        t_stat, p_val = ttest_ind(x2, x1, equal_var=False) # Use independent t-test\n",
        "        paired = False # Ensure summary is correct\n",
        "    else: # Original paired test logic\n",
        "        print(\"Performing paired t-test.\")\n",
        "        if subject_col not in df.columns: # Added check\n",
        "             raise ValueError(f\"Subject column '{subject_col}' not found in DataFrame for paired test.\")\n",
        "        df_pivot = df.pivot(index=subject_col, columns=condition_col, values=gene).dropna()\n",
        "        x1 = df_pivot[cond1].values\n",
        "        x2 = df_pivot[cond2].values\n",
        "        t_stat, p_val = ttest_rel(x2, x1)\n",
        "\n",
        "    # Adjust for one-sided alternatives\n",
        "    if alternative == \"both\":\n",
        "        pass  # already correct\n",
        "    elif alternative == \"pos\":  # treated > control\n",
        "        if t_stat > 0:\n",
        "            p_val /= 2\n",
        "        else:\n",
        "            p_val = 1 - (1 - p_val / 2)\n",
        "    elif alternative == \"neg\":  # treated < control\n",
        "        if t_stat < 0:\n",
        "            p_val /= 2\n",
        "        else:\n",
        "            p_val = 1 - (1 - p_val / 2)\n",
        "    else:\n",
        "        raise ValueError(\"alternative must be one of: 'both', 'pos', 'neg'\")\n",
        "\n",
        "    summary = f\"T-test ({'paired' if paired else 'unpaired'}, {alternative}): {cond2} vs {cond1} â€” t = {t_stat:.3f}, p = {p_val:.3e}\"\n",
        "    return t_stat, p_val, summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rMwGfRz2kK4b",
      "metadata": {
        "id": "rMwGfRz2kK4b"
      },
      "source": [
        "#### T-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "LxLP75EukIzY",
      "metadata": {
        "id": "LxLP75EukIzY"
      },
      "outputs": [],
      "source": [
        "def paired_ttest(pb_df, cond1='untreated', cond2='treated',\n",
        "                               expr_threshold=0.1, fold_change_threshold=1.5,\n",
        "                               group_col='treatment_group', subject_col='patient'):\n",
        "    \"\"\"\n",
        "    Perform a fast, batched paired one-sided t-test on pseudobulk expression data.\n",
        "    Filters genes expressed in at least `expr_threshold` fraction of subjects in cond2\n",
        "    and with fold change â‰¥ `fold_change_threshold`.\n",
        "    \"\"\"\n",
        "    from scipy.stats import ttest_rel\n",
        "    import pandas as pd\n",
        "\n",
        "    # Pivot to subject Ã— (condition, gene) matrix\n",
        "    wide_df = pb_df.pivot(index=subject_col, columns=group_col)\n",
        "\n",
        "    # Get all genes (exclude multi-index level 0 which is 'expr')\n",
        "    all_genes = wide_df.columns.levels[1]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for gene in all_genes:\n",
        "        try:\n",
        "            x1 = wide_df[cond1][gene]\n",
        "            x2 = wide_df[cond2][gene]\n",
        "        except KeyError:\n",
        "            continue  # Skip genes missing in either group\n",
        "\n",
        "        # Drop subjects with missing data in either condition\n",
        "        valid_mask = x1.notna() & x2.notna()\n",
        "        x1 = x1[valid_mask]\n",
        "        x2 = x2[valid_mask]\n",
        "\n",
        "        if len(x1) < 3:\n",
        "            continue\n",
        "\n",
        "        # Expression threshold: at least X% of subjects in cond2 express the gene\n",
        "        expr_pct = (x2 > 0).sum() / len(x2)\n",
        "        if expr_pct < expr_threshold:\n",
        "            continue\n",
        "\n",
        "        # Fold change\n",
        "        fc = (x2.mean() + 1e-6) / (x1.mean() + 1e-6)\n",
        "        if fc < fold_change_threshold:\n",
        "            continue\n",
        "\n",
        "        # Paired one-sided t-test (cond2 > cond1)\n",
        "        stat, pval = ttest_rel(x2, x1, alternative='greater')\n",
        "\n",
        "        results.append({\n",
        "            'gene': gene,\n",
        "            'fold_change': fc,\n",
        "            'expr_pct_treated': expr_pct,\n",
        "            'pval': pval,\n",
        "            'n_pairs': len(x1)\n",
        "\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).sort_values('pval')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aolL2wz4T1qb",
      "metadata": {
        "id": "aolL2wz4T1qb"
      },
      "source": [
        "#### Fetch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6IUVS-x8Qhyn",
      "metadata": {
        "id": "6IUVS-x8Qhyn"
      },
      "outputs": [],
      "source": [
        "def fetch(filenames: list):\n",
        "  \"\"\"Downloads a list of files from the specified GCS bucket.\n",
        "\n",
        "  Args:\n",
        "    filenames: A list of strings, where each string is a filename in the GCS\n",
        "      bucket.\n",
        "  \"\"\"\n",
        "  if isinstance(filenames, str):\n",
        "    filenames = [filenames]\n",
        "  for filename in filenames:\n",
        "    # Extract only the base filename\n",
        "    base_filename = os.path.basename(filename)\n",
        "    blob = bucket.blob(filename)\n",
        "    # Download to /content/ with just the base filename\n",
        "    blob.download_to_filename(f'/content/{base_filename}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TU0eKzLLuzJn",
      "metadata": {
        "id": "TU0eKzLLuzJn"
      },
      "source": [
        "#### Cell time course"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "lQCie1NxuwNQ",
      "metadata": {
        "id": "lQCie1NxuwNQ"
      },
      "outputs": [],
      "source": [
        "def plot_timecourse(\n",
        "    adata,\n",
        "    features,\n",
        "    time_key='week',         # Must be numeric (e.g. 0, 1, 2, ...)\n",
        "    patient_key='patient_id',\n",
        "    split_by=None,\n",
        "    color_by=None,\n",
        "    sem=True,\n",
        "    relative=False,\n",
        "    figsize=(10, 6),\n",
        "    ylim=(None, None),\n",
        "    ymin_zero=True\n",
        "):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    for feature in features:\n",
        "        # Extract expression\n",
        "        if feature in adata.obs.columns:\n",
        "            vals = adata.obs[feature]\n",
        "        elif adata.raw is not None and feature in adata.raw.var_names:\n",
        "            arr = (\n",
        "                adata.raw[:, feature].X.toarray().ravel()\n",
        "                if sp.issparse(adata.raw.X)\n",
        "                else adata.raw[:, feature].X.ravel()\n",
        "            )\n",
        "            vals = pd.Series(arr, index=adata.obs_names, name=feature)\n",
        "        else:\n",
        "            raise KeyError(f\"Feature '{feature}' not found in obs or raw.\")\n",
        "\n",
        "        # Build base dataframe\n",
        "        df = pd.DataFrame({\n",
        "            time_key: pd.to_numeric(adata.obs[time_key], errors='coerce'),\n",
        "            patient_key: adata.obs[patient_key].astype(str),\n",
        "            feature: vals.values\n",
        "        }).dropna(subset=[time_key])\n",
        "        if split_by:\n",
        "            df[split_by] = adata.obs[split_by].astype(str)\n",
        "        if color_by:\n",
        "            df[color_by] = adata.obs[color_by].astype(str)\n",
        "\n",
        "        group_cols = [time_key, split_by] if split_by else [time_key]\n",
        "\n",
        "        # Step 1: Aggregate per patient\n",
        "        patient_means = (\n",
        "            df.groupby([time_key, patient_key] + ([split_by] if split_by else []), observed=True)[feature]\n",
        "              .mean()\n",
        "              .reset_index()\n",
        "        )\n",
        "\n",
        "        # Step 2: Compute stats across patients\n",
        "        stats = (\n",
        "            patient_means\n",
        "            .groupby(group_cols, observed=True)[feature]\n",
        "            .agg(mean='mean', std='std', count='count')\n",
        "            .reset_index()\n",
        "        )\n",
        "        stats['sem'] = stats['std'] / np.sqrt(stats['count'])\n",
        "        if split_by:\n",
        "            stats[split_by] = stats[split_by].astype(str)\n",
        "\n",
        "        # Optional: Add color_by (e.g. genotype)\n",
        "        if color_by:\n",
        "            color_lookup = (\n",
        "                df.groupby(group_cols, observed=True)[color_by]\n",
        "                  .agg(lambda x: x.mode().iloc[0])\n",
        "                  .reset_index()\n",
        "            )\n",
        "            stats = stats.merge(color_lookup, on=group_cols, how='left')\n",
        "            categories = sorted(df[color_by].unique())\n",
        "            palette = sns.color_palette(\"Set2\", len(categories))\n",
        "            color_dict = dict(zip(categories, palette))\n",
        "\n",
        "        # Relative expression (% of baseline)\n",
        "        if relative:\n",
        "            if split_by:\n",
        "                stats['mean'] = stats.groupby(split_by)['mean'].transform(\n",
        "                    lambda x: (x / x.iloc[0]) * 100 if x.iloc[0] > 0 else 0\n",
        "                )\n",
        "                stats['sem'] = stats.groupby(split_by)['sem'].transform(\n",
        "                    lambda x: (x / x.iloc[0]) * 100 if x.iloc[0] > 0 else 0\n",
        "                )\n",
        "            else:\n",
        "                base = stats['mean'].iloc[0]\n",
        "                stats['mean'] = (stats['mean'] / base) * 100 if base > 0 else 0\n",
        "                stats['sem'] = (stats['sem'] / base) * 100 if base > 0 else 0\n",
        "            y_label = 'Relative mean expression (%)'\n",
        "        else:\n",
        "            y_label = \"Mean Â± SEM\" if sem else \"Mean\"\n",
        "\n",
        "        # Plotting\n",
        "        if split_by:\n",
        "            for _, subdf in stats.groupby(split_by, observed=True):\n",
        "                subdf = subdf.sort_values(by=time_key)\n",
        "                label = subdf[split_by].iloc[0]\n",
        "                color_val = subdf[color_by].iloc[0] if color_by else None\n",
        "                color = color_dict[color_val] if color_by else None\n",
        "\n",
        "                ax.errorbar(\n",
        "                    subdf[time_key],\n",
        "                    subdf['mean'],\n",
        "                    yerr=subdf['sem'] if sem else None,\n",
        "                    marker='o',\n",
        "                    linestyle='-',\n",
        "                    label=label,\n",
        "                    linewidth=1,\n",
        "                    color=color\n",
        "                )\n",
        "        else:\n",
        "            stats = stats.sort_values(by=time_key)\n",
        "            label = feature\n",
        "            color_val = stats[color_by].iloc[0] if color_by else None\n",
        "            color = color_dict[color_val] if color_by else None\n",
        "\n",
        "            ax.errorbar(\n",
        "                stats[time_key],\n",
        "                stats['mean'],\n",
        "                yerr=stats['sem'] if sem else None,\n",
        "                marker='o',\n",
        "                linestyle='-',\n",
        "                label=label,\n",
        "                linewidth=1,\n",
        "                color=color\n",
        "            )\n",
        "\n",
        "    # Legend\n",
        "    legend_title = split_by.capitalize() if split_by else \"Feature\"\n",
        "    ax.legend(title=legend_title, loc=\"upper left\", bbox_to_anchor=(1.02, 1))\n",
        "\n",
        "    # Labels and styling\n",
        "    ax.set_xlabel(time_key)\n",
        "    ax.set_ylabel(y_label)\n",
        "    title = f\"Time course of {features[0]}\" if len(features) == 1 else \"Time course of features\"\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Y-axis control\n",
        "    if ymin_zero:\n",
        "        ax.set_ylim(bottom=0, top=ylim[1])\n",
        "    else:\n",
        "        ax.set_ylim(ylim)\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e201de0-234f-454f-8a5b-33ef63508f63",
      "metadata": {
        "id": "5e201de0-234f-454f-8a5b-33ef63508f63"
      },
      "source": [
        "#### Find Markers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6caab6e1-e885-4545-9f20-1c5a3841d80f",
      "metadata": {
        "id": "6caab6e1-e885-4545-9f20-1c5a3841d80f"
      },
      "outputs": [],
      "source": [
        "def markers(cell_type, sign='pos', markers_yaml=\"markers.yaml\", ontology_yaml=\"ontology.yaml\", adata=None):\n",
        "    if sign not in {'pos', 'neg'}:\n",
        "        raise ValueError(\"sign must be 'pos' or 'neg'\")\n",
        "\n",
        "    # Re-load and normalize markers from YAML files\n",
        "    _, normalized_markers = init_ontology(ontology_yaml, markers_yaml, root=\"root\")\n",
        "\n",
        "    if cell_type not in normalized_markers:\n",
        "        print(f\"âš ï¸ Cell type '{cell_type}' not found in marker definitions. Skipping.\")\n",
        "        return []\n",
        "\n",
        "    markers_raw = normalized_markers[cell_type][sign]\n",
        "\n",
        "    if adata is not None:\n",
        "        var_names = adata.raw.var_names if getattr(adata, 'raw', None) is not None else adata.var_names\n",
        "        present = [g for g in markers_raw if g in var_names]\n",
        "        missing = [g for g in markers_raw if g not in var_names]\n",
        "        for g in missing:\n",
        "            print(f\"âš ï¸ Marker '{g}' not found in data. Skipping.\")\n",
        "        return present\n",
        "\n",
        "    return markers_raw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oTDMgSB0GhkK",
      "metadata": {
        "id": "oTDMgSB0GhkK"
      },
      "source": [
        "#### Analyze cell type distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "_fZpd03PGckg",
      "metadata": {
        "id": "_fZpd03PGckg"
      },
      "outputs": [],
      "source": [
        "def analyze_cell_type_distribution(\n",
        "    adata,\n",
        "    groupby='timepoint',\n",
        "    control_group='Control',\n",
        "    key_added='FastTag',\n",
        "    sample='patient',\n",
        "    paired=False\n",
        "):\n",
        "\n",
        "    df = adata.obs[[key_added, sample, groupby]].copy()\n",
        "\n",
        "    # Sanity check: control group exists\n",
        "    group_vals = df[groupby].unique()\n",
        "    if control_group not in group_vals:\n",
        "        raise ValueError(f\"control_group '{control_group}' not found in '{groupby}' values: {group_vals}\")\n",
        "\n",
        "    # Count and percent cell types per sample\n",
        "    counts = df.groupby([sample, groupby, key_added], observed=True).size().unstack(fill_value=0)\n",
        "    pct = counts.div(counts.sum(axis=1), axis=0) * 100\n",
        "    pct = pct.sort_index(level=[groupby, sample])\n",
        "\n",
        "    # Reorder cell types\n",
        "    cell_types = list(pct.columns)\n",
        "    if 'Treg' in cell_types:\n",
        "        cell_types.remove('Treg')\n",
        "        cell_types = ['Treg'] + cell_types\n",
        "\n",
        "        # Plot stacked barplot with grouped samples and spacing\n",
        "    fig, ax = plt.subplots(figsize=(40, 10))\n",
        "    colors = plt.cm.tab20.colors\n",
        "\n",
        "    # Extract group/sample info\n",
        "    index_df = pct.index.to_frame(index=False)\n",
        "    groups = index_df[groupby].unique()\n",
        "    spacer = 1  # gap between groups\n",
        "\n",
        "    bar_positions = []\n",
        "    xtick_positions = []\n",
        "    xtick_labels = []\n",
        "    plot_data = []\n",
        "\n",
        "    pos = 0\n",
        "    for grp in groups:\n",
        "        sub = pct.loc[pct.index.get_level_values(groupby) == grp]\n",
        "        n = len(sub)\n",
        "        bar_positions.extend(range(pos, pos + n))\n",
        "        xtick_positions.append(pos + n / 2 - 0.5)\n",
        "        xtick_labels.append(grp)\n",
        "        plot_data.append(sub)\n",
        "        # spacer row (NaNs â†’ zeros later)\n",
        "        plot_data.append(pd.DataFrame(np.nan, index=range(spacer), columns=sub.columns))\n",
        "        pos += n + spacer\n",
        "\n",
        "    stacked_df = pd.concat(plot_data, ignore_index=True).fillna(0)\n",
        "\n",
        "    bottom = np.zeros(len(stacked_df))\n",
        "    for i, ct in enumerate(cell_types):\n",
        "        ax.bar(range(len(stacked_df)), stacked_df[ct].values, bottom=bottom,\n",
        "               label=ct, color=colors[i % len(colors)])\n",
        "        bottom += stacked_df[ct].values\n",
        "\n",
        "    ax.set_xticks(xtick_positions)\n",
        "    ax.set_xticklabels(xtick_labels, fontsize=16)\n",
        "    ax.set_ylabel('Percentage of Cells (%)')\n",
        "    ax.legend(title='Cell Type', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=15)\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a3344a-3d5f-411b-adda-eb3192f454d9",
      "metadata": {
        "id": "d3a3344a-3d5f-411b-adda-eb3192f454d9"
      },
      "source": [
        "#### Cell count table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6672278a-6a55-4577-8758-9c973b34cf36",
      "metadata": {
        "id": "6672278a-6a55-4577-8758-9c973b34cf36"
      },
      "outputs": [],
      "source": [
        "def cell_count_table(\n",
        "    adata,\n",
        "    row='FastTag',\n",
        "    column=None,\n",
        "    show_sample_counts=False,\n",
        "    sample_col='donor_id',\n",
        "    fill_value='0',\n",
        "    display_full=False,\n",
        "):\n",
        "    # Grouping variables\n",
        "    group_keys = [row] if column is None else [row, column]\n",
        "\n",
        "    # Count cells\n",
        "    cell_counts = (\n",
        "        adata.obs\n",
        "        .groupby(group_keys, observed=True)\n",
        "        .size()\n",
        "        .reset_index(name='cell_count')\n",
        "    )\n",
        "\n",
        "    if show_sample_counts:\n",
        "        sample_counts = (\n",
        "            adata.obs\n",
        "            .groupby(group_keys, observed=True)[sample_col]\n",
        "            .nunique()\n",
        "            .reset_index(name='sample_count')\n",
        "        )\n",
        "        merged = pd.merge(cell_counts, sample_counts, on=group_keys)\n",
        "        merged['info'] = (\n",
        "            merged['cell_count'].astype(str) +\n",
        "            ' (' + merged['sample_count'].astype(str) + ')'\n",
        "        )\n",
        "        value_col = 'info'\n",
        "    else:\n",
        "        merged = cell_counts\n",
        "        merged['info'] = merged['cell_count'].astype(str)\n",
        "        value_col = 'info'\n",
        "\n",
        "    if column is None:\n",
        "        table = merged.set_index(row)[value_col]\n",
        "    else:\n",
        "        table = merged.pivot_table(\n",
        "            index=row,\n",
        "            columns=column,\n",
        "            values=value_col,\n",
        "            fill_value=fill_value,\n",
        "            aggfunc='first',\n",
        "            observed=True\n",
        "        )\n",
        "\n",
        "    if display_full:\n",
        "        pd.set_option('display.max_rows', None)\n",
        "        display(table)\n",
        "    else:\n",
        "        print(table)\n",
        "        print(f\"Total cell count: {cell_counts['cell_count'].sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42L-izp3xCf5",
      "metadata": {
        "id": "42L-izp3xCf5"
      },
      "source": [
        "#### Volcano auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "HCWFJ3sfw-ns",
      "metadata": {
        "id": "HCWFJ3sfw-ns"
      },
      "outputs": [],
      "source": [
        "def volcano_auc_split(\n",
        "    df: pd.DataFrame,\n",
        "    fold_change_col: str = 'log2fc',\n",
        "    pval_col: str = 'pval',\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Compute separate AUCs for upregulated and downregulated genes from a volcano plot.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    auc_up : float\n",
        "        AUC over genes with log2FC > 0\n",
        "    auc_down : float\n",
        "        AUC over genes with log2FC < 0\n",
        "    \"\"\"\n",
        "    sub = df[[fold_change_col, pval_col]].dropna().copy()\n",
        "    sub = sub[sub[pval_col] > 0]\n",
        "    sub['neglog10p'] = -np.log10(sub[pval_col])\n",
        "\n",
        "    # Sort and split\n",
        "    sub = sub.sort_values(fold_change_col)\n",
        "    up = sub[sub[fold_change_col] > 0]\n",
        "    down = sub[sub[fold_change_col] < 0]\n",
        "\n",
        "    auc_up = np.trapezoid(up['neglog10p'], up[fold_change_col]) if not up.empty else 0.0\n",
        "    auc_down = np.trapezoid(down['neglog10p'], down[fold_change_col]) if not down.empty else 0.0\n",
        "\n",
        "    return auc_up, auc_down\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b3082d3-dbd2-44ce-aaff-6869abf64eaf",
      "metadata": {
        "id": "2b3082d3-dbd2-44ce-aaff-6869abf64eaf"
      },
      "source": [
        "#### Save cell dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2d489f8d-34d5-4980-b2f6-adb0ff4b60d8",
      "metadata": {
        "id": "2d489f8d-34d5-4980-b2f6-adb0ff4b60d8"
      },
      "outputs": [],
      "source": [
        "def save_dict(adata_dict, output_path, compression='gzip', compression_opts=1):\n",
        "    adata_list = []\n",
        "\n",
        "    for category, adata in adata_dict.items():\n",
        "        # Ensure in-memory AnnData (avoid .backed mode)\n",
        "        if getattr(adata, 'isbacked', False):\n",
        "            adata = adata.to_memory()\n",
        "\n",
        "        adata = adata.copy()\n",
        "        adata.obs['cell_category'] = category\n",
        "        adata_list.append(adata)\n",
        "\n",
        "    # Concatenate all objects\n",
        "    merged = ad.concat(\n",
        "        adata_list,\n",
        "        axis=0,\n",
        "        join='outer',\n",
        "        label=None,\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    # Save with compression (if HDF5 format)\n",
        "    merged.write(output_path, compression=compression, compression_opts=compression_opts)\n",
        "\n",
        "    return merged\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a034d89c-3a05-484d-bba1-8e9a55ef781b",
      "metadata": {
        "id": "a034d89c-3a05-484d-bba1-8e9a55ef781b"
      },
      "source": [
        "#### Downsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c5d5c72d-0157-4741-8d5e-010526289a1e",
      "metadata": {
        "id": "c5d5c72d-0157-4741-8d5e-010526289a1e"
      },
      "outputs": [],
      "source": [
        "def downsample (adata, groupby, n_cells):\n",
        "    sampled_indices = (\n",
        "        adata.obs\n",
        "        .groupby(groupby, group_keys=False)\n",
        "        .apply(lambda x: x.sample(min(n_cells, len(x)), random_state=0))\n",
        "        .index\n",
        "    )\n",
        "    return adata[sampled_indices].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21125aae-4e61-45ac-a582-9065e5075dd6",
      "metadata": {
        "id": "21125aae-4e61-45ac-a582-9065e5075dd6"
      },
      "source": [
        "#### Subset data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "593a7d89-d324-4950-bbcb-4e270a07cf7e",
      "metadata": {
        "id": "593a7d89-d324-4950-bbcb-4e270a07cf7e"
      },
      "outputs": [],
      "source": [
        "def subset(adata, column: str, value) -> 'AnnData':\n",
        "    if adata.raw is None:\n",
        "        raise ValueError('AnnData.raw is not set. Cannot subset raw data.')\n",
        "\n",
        "    if column not in adata.obs.columns:\n",
        "        raise KeyError(f\"Column '{column}' not found in adata.obs.\")\n",
        "\n",
        "    mask = adata.obs[column] == value\n",
        "    if mask.sum() == 0:\n",
        "        raise ValueError(f\"No cells found where adata.obs['{column}'] == {value!r}\")\n",
        "\n",
        "    adata_sub = adata[mask].copy()\n",
        "    adata_sub.raw = adata.raw.to_adata()[mask].copy()\n",
        "    return adata_sub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4dc595-9f8d-4690-bfb6-c2f5ce856265",
      "metadata": {
        "id": "5a4dc595-9f8d-4690-bfb6-c2f5ce856265"
      },
      "source": [
        "#### Purge rare cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b49f3529-2cbf-457e-a99b-a6bf856dd9ca",
      "metadata": {
        "id": "b49f3529-2cbf-457e-a99b-a6bf856dd9ca"
      },
      "outputs": [],
      "source": [
        "def purge(adata, column, min_count=10):\n",
        "    if column not in adata.obs:\n",
        "        raise ValueError(f\"Column '{column}' not found in adata.obs.\")\n",
        "\n",
        "    # Count instances per category\n",
        "    counts = adata.obs[column].value_counts()\n",
        "\n",
        "    # Determine valid (frequent enough) categories\n",
        "    valid_categories = counts[counts >= min_count].index\n",
        "\n",
        "    # Create boolean mask for cells to keep\n",
        "    keep_mask = adata.obs[column].isin(valid_categories)\n",
        "\n",
        "    # Apply mask to keep only relevant cells\n",
        "    adata._inplace_subset_obs(keep_mask)\n",
        "\n",
        "    # Clean up unused categories if applicable\n",
        "    if pd.api.types.is_categorical_dtype(adata.obs[column]):\n",
        "        adata.obs[column] = adata.obs[column].cat.remove_unused_categories()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d39391a-8f22-46b4-9330-9073e56e816e",
      "metadata": {
        "id": "8d39391a-8f22-46b4-9330-9073e56e816e"
      },
      "source": [
        "#### Convert to HGNC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "92513e13-4f03-47ba-bdb2-80382da1bb03",
      "metadata": {
        "id": "92513e13-4f03-47ba-bdb2-80382da1bb03"
      },
      "outputs": [],
      "source": [
        "def convert(adata):\n",
        "    \"\"\"\n",
        "    Convert var_names (and raw.var_names) from Ensembl IDs or aliases to official gene symbols.\n",
        "    Stores original names in adata.var['original_gene'] and, if raw exists, in adata.raw.var['original_gene'].\n",
        "    \"\"\"\n",
        "    mg_client = mg.MyGeneInfo()\n",
        "\n",
        "    # Guess ID type on a small sample\n",
        "    sample = adata.var_names[:20]\n",
        "    if any(re.match(r'ENSG\\d{9}', g) for g in sample):\n",
        "        print('ðŸ”„ Looks like Ensembl IDs â€” converting to gene symbols.')\n",
        "        query_args = dict(scopes='ensembl.gene', fields='symbol', species='human')\n",
        "    else:\n",
        "        print('ðŸ”„ Looks like gene symbols â€” converting aliases to official symbols.')\n",
        "        query_args = dict(scopes='symbol,alias', fields='symbol', species='human')\n",
        "\n",
        "    # Query MyGeneInfo\n",
        "    results = mg_client.querymany(adata.var_names.tolist(), **query_args)\n",
        "    mapping = {entry['query']: entry.get('symbol', entry['query']) for entry in results}\n",
        "\n",
        "    # Backup original var_names in var\n",
        "    adata.var['original_gene'] = adata.var_names.copy()\n",
        "\n",
        "    # Map var_names\n",
        "    new_names = adata.var_names.map(lambda g: mapping.get(g, g))\n",
        "    adata.var_names = new_names\n",
        "    adata.var_names_make_unique()\n",
        "\n",
        "    # Also update raw.var_names if raw exists\n",
        "    if adata.raw is not None:\n",
        "        # Backup raw original names\n",
        "        adata.raw.var['original_gene'] = adata.raw.var_names.copy()\n",
        "        # Apply new names to raw.var\n",
        "        adata.raw.var.index = adata.var_names.copy()\n",
        "\n",
        "    num_changed = (adata.var['original_gene'] != adata.var_names).sum()\n",
        "    print(f\"âœ… Conversion complete. {num_changed} gene names were changed.\")\n",
        "    return adata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "535f8581-c1b6-43d3-9fb9-558822a8830c",
      "metadata": {
        "id": "535f8581-c1b6-43d3-9fb9-558822a8830c"
      },
      "source": [
        "#### Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5302b13f-85cb-49de-af92-ebb44b5fa144",
      "metadata": {
        "id": "5302b13f-85cb-49de-af92-ebb44b5fa144"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split(adata, category, min_cells=20, skip_unknown=True):\n",
        "    labels = adata.obs[category].unique()\n",
        "    subsets = {}\n",
        "\n",
        "    for label in labels:\n",
        "        if skip_unknown and label == 'Unknown':\n",
        "            continue\n",
        "\n",
        "        ad_subset = adata[adata.obs[category] == label].copy()\n",
        "        if ad_subset.n_obs >= min_cells:\n",
        "            subsets[label] = ad_subset\n",
        "\n",
        "    return subsets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277b4a6b-6097-47f1-a763-fec6097c9a11",
      "metadata": {
        "id": "277b4a6b-6097-47f1-a763-fec6097c9a11"
      },
      "source": [
        "#### Enrich"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b688b1b6-f15d-4913-b6dc-c8a7b00f98af",
      "metadata": {
        "id": "b688b1b6-f15d-4913-b6dc-c8a7b00f98af"
      },
      "outputs": [],
      "source": [
        "def enrich_all(genes, description='Example gene list'):\n",
        "    library_list = [\n",
        "        'Azimuth_2023',\n",
        "        'CellMarker_2024',\n",
        "        'HuBMAP_ASCTplusB_augmented_2022',\n",
        "        'PanglaoDB_Augmented_2021',\n",
        "        'Tabula_Sapiens'\n",
        "    ]\n",
        "    # library_list = [\n",
        "    #     'Reactome_Pathways_2024',\n",
        "    #     'WikiPathways_2024_Human',\n",
        "    #     'BioPlanet_2019',\n",
        "    #     'KEGG_2021_Human',\n",
        "    #     'Elsevier_Pathway_Collection'\n",
        "    # ]\n",
        "\n",
        "    combined_results = []\n",
        "\n",
        "    for library_name in library_list:\n",
        "        time.sleep(1)  # Avoid rate limiting\n",
        "        payload = {'list': '\\n'.join(genes), 'description': description}\n",
        "        response = requests.post('https://maayanlab.cloud/Enrichr/addList', files=payload)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise Exception('Error analyzing gene list:', response.text)\n",
        "\n",
        "        user_list_id = response.json().get('userListId')\n",
        "        results_url = f'https://maayanlab.cloud/Enrichr/enrich?userListId={user_list_id}&backgroundType={library_name}'\n",
        "        enrichment_response = requests.get(results_url)\n",
        "\n",
        "        if enrichment_response.status_code != 200:\n",
        "            raise Exception('Error fetching enrichment results:', enrichment_response.text)\n",
        "\n",
        "        enrichment_data = enrichment_response.json().get(library_name, [])\n",
        "        if not enrichment_data:\n",
        "            continue\n",
        "\n",
        "        # Convert to dataframe\n",
        "        results_df = pd.DataFrame([\n",
        "            {\n",
        "                'Library': library_name,\n",
        "                'Term': row[1],\n",
        "                'PValue': float(row[2]),\n",
        "                'AdjustedPValue': float(row[6]),\n",
        "                'OverlappingGenes': ', '.join(row[5])\n",
        "            }\n",
        "            for row in enrichment_data\n",
        "        ])\n",
        "\n",
        "        combined_results.append(results_df)\n",
        "\n",
        "    if not combined_results:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    combined_df = pd.concat(combined_results, ignore_index=True)\n",
        "\n",
        "    # Filter results\n",
        "    combined_df = combined_df[combined_df['AdjustedPValue'] < 0.01]\n",
        "    combined_df = combined_df[combined_df['OverlappingGenes'].str.split(', ').map(len) > 3]\n",
        "\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e469ca0b-e673-4e72-83f8-d95ed6982ed1",
      "metadata": {
        "id": "e469ca0b-e673-4e72-83f8-d95ed6982ed1"
      },
      "source": [
        "#### ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ac6bdae2-4fd7-4ce3-a16b-d6cc02fdd8ae",
      "metadata": {
        "id": "ac6bdae2-4fd7-4ce3-a16b-d6cc02fdd8ae"
      },
      "outputs": [],
      "source": [
        "def ChatGPT(gene_set, cell_category, tissue, model='gpt-4o'):\n",
        "    # Construct the prompt\n",
        "    prompt = f'''\n",
        "You are a biomedical expert. Based on your knowledge of gene expression and cellular specialization,\n",
        "assess which {cell_category} in the {tissue} are most likely to differentially express the following gene set:\n",
        "\n",
        "{', '.join(gene_set)}\n",
        "\n",
        "Consider the gene set as a whole. Do not assess each gene individually.\n",
        "\n",
        "List only the relevant cell types, each with a summary likelihood:\n",
        "'very likely', 'possibly', or 'unlikely'.\n",
        "\n",
        "Present your answer in this format:\n",
        "\n",
        "- Cell_Type_A: very likely\n",
        "- Cell_Type_B: possibly\n",
        "'''\n",
        "\n",
        "    # Query ChatGPT\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{'role': 'user', 'content': prompt}],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Extract and parse text response\n",
        "    text = response.choices[0].message.content\n",
        "    lines = text.splitlines()\n",
        "\n",
        "    # Filter and parse into tuples\n",
        "    parsed = []\n",
        "    for line in lines:\n",
        "        if ':' in line:\n",
        "            cell_type, likelihood = line.split(':', 1)\n",
        "            parsed.append((cell_type.strip(' -*â€¢'), likelihood.strip().lower()))\n",
        "\n",
        "    # Sort by likelihood level\n",
        "    likelihood_order = {'very likely': 0, 'possibly': 1, 'unlikely': 2}\n",
        "    parsed_sorted = sorted(parsed, key=lambda x: likelihood_order.get(x[1], 3))\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(parsed_sorted, columns=['Cell Type', 'Likelihood'])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c6aafab-5e4f-41e3-947d-37df85b3df3f",
      "metadata": {
        "id": "7c6aafab-5e4f-41e3-947d-37df85b3df3f"
      },
      "source": [
        "#### DGE computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "56f97196-236d-4032-92ca-5f5fb6366e4f",
      "metadata": {
        "id": "56f97196-236d-4032-92ca-5f5fb6366e4f"
      },
      "outputs": [],
      "source": [
        "def DGE(\n",
        "    adata,\n",
        "    target_group,\n",
        "    reference_group,\n",
        "    groupby,\n",
        "    min_pct=None,\n",
        "    min_log2fc=None,\n",
        "    min_delta_pct=None,\n",
        "    direction='pos',  # 'pos', 'neg', or 'both'\n",
        "    n_genes=200,\n",
        "    n_sample=None,\n",
        "    return_df=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Differential Gene Expression for log-transformed data with optional Î”pct and fold-change filters.\n",
        "    Performs Welch's t-test and FDR correction. Assumes adata.raw is log-normalized data.\n",
        "    \"\"\"\n",
        "\n",
        "    if direction not in ('pos', 'neg', 'both'):\n",
        "        raise ValueError(\"direction must be one of 'pos', 'neg', or 'both'\")\n",
        "    if adata.raw is None:\n",
        "        raise ValueError(\"`adata.raw` must be set before running DGE().\")\n",
        "    if isinstance(target_group, str):\n",
        "        target_group = [target_group]\n",
        "\n",
        "    labels = adata.obs[groupby].values\n",
        "    tmask = np.isin(labels, target_group)\n",
        "    rmask = ~tmask if reference_group == 'rest' else (labels == reference_group)\n",
        "\n",
        "    def subsample(mask):\n",
        "        idx = np.where(mask)[0]\n",
        "        if n_sample is not None and len(idx) > n_sample:\n",
        "            return np.random.choice(idx, size=n_sample, replace=False)\n",
        "        return idx\n",
        "\n",
        "    t_idx = subsample(tmask)\n",
        "    r_idx = subsample(rmask)\n",
        "    X = adata.raw.X\n",
        "    var_names = adata.raw.var_names\n",
        "    X_t = X[t_idx]\n",
        "    X_r = X[r_idx]\n",
        "    nt, nr = len(t_idx), len(r_idx)\n",
        "\n",
        "    # Means and % expressing\n",
        "    if sp.issparse(X):\n",
        "        mean_t = X_t.mean(axis=0).A1\n",
        "        mean_r = X_r.mean(axis=0).A1\n",
        "        pct_t = X_t.getnnz(axis=0) / nt\n",
        "        pct_r = X_r.getnnz(axis=0) / nr\n",
        "        X_t_dense = X_t.toarray()\n",
        "        X_r_dense = X_r.toarray()\n",
        "    else:\n",
        "        mean_t = X_t.mean(axis=0)\n",
        "        mean_r = X_r.mean(axis=0)\n",
        "        pct_t = (X_t > 0).mean(axis=0)\n",
        "        pct_r = (X_r > 0).mean(axis=0)\n",
        "        X_t_dense = X_t\n",
        "        X_r_dense = X_r\n",
        "\n",
        "    log2fc = mean_t - mean_r\n",
        "    delta_pct = pct_t - pct_r\n",
        "\n",
        "    # Filter before statistical test\n",
        "    if direction == 'both':\n",
        "        fc_mask = np.abs(log2fc) >= (min_log2fc or 0)\n",
        "        dp_mask = np.abs(delta_pct) >= (min_delta_pct or 0)\n",
        "    elif direction == 'pos':\n",
        "        fc_mask = log2fc >= (min_log2fc or 0)\n",
        "        dp_mask = delta_pct >= (min_delta_pct or 0)\n",
        "    else:  # 'neg'\n",
        "        fc_mask = log2fc <= -(min_log2fc or 0)\n",
        "        dp_mask = delta_pct <= -(min_delta_pct or 0)\n",
        "\n",
        "    pct_mask = pct_t >= (min_pct or 0)\n",
        "    keep_mask = np.logical_and.reduce([pct_mask, fc_mask, dp_mask])\n",
        "    kept = np.where(keep_mask)[0]\n",
        "\n",
        "    if kept.size == 0:\n",
        "        print(\"âš ï¸ No genes passed the filtering criteria.\")\n",
        "        return pd.DataFrame(columns=[\n",
        "            'gene', 'mean_target', 'mean_ref',\n",
        "            'pct_target', 'pct_ref', 'delta_pct',\n",
        "            'log2fc', 'pval', 'pval_adj'\n",
        "        ]) if return_df else []\n",
        "\n",
        "    # Run t-tests only on filtered genes\n",
        "    X_t_dense = X_t_dense[:, kept]\n",
        "    X_r_dense = X_r_dense[:, kept]\n",
        "    _, pvals = ttest_ind(X_t_dense, X_r_dense, axis=0, equal_var=False)\n",
        "    pvals = np.asarray(pvals)\n",
        "\n",
        "    # Adjust p-values\n",
        "    adj_pvals = np.full_like(pvals, np.nan, dtype=np.float64)\n",
        "    valid = ~np.isnan(pvals)\n",
        "    if valid.sum() > 0:\n",
        "        adj_pvals[valid] = multipletests(pvals[valid], method='fdr_bh')[1]\n",
        "    else:\n",
        "        print(\"âš ï¸ All p-values are NaN â€” no FDR correction performed.\")\n",
        "\n",
        "    # Assemble results\n",
        "    df = pd.DataFrame({\n",
        "        'gene': var_names[kept],\n",
        "        'mean_target': mean_t[kept],\n",
        "        'mean_ref': mean_r[kept],\n",
        "        'pct_target': pct_t[kept],\n",
        "        'pct_ref': pct_r[kept],\n",
        "        'delta_pct': delta_pct[kept],\n",
        "        'log2fc': log2fc[kept],\n",
        "        'pval': pvals,\n",
        "        'pval_adj': adj_pvals,\n",
        "    })\n",
        "\n",
        "    # Sort genes\n",
        "    if direction == 'both':\n",
        "        df = df.sort_values('pval')\n",
        "    elif direction == 'neg':\n",
        "        df = df.sort_values(['log2fc', 'pval'], ascending=[True, True])\n",
        "    else:\n",
        "        df = df.sort_values(['log2fc', 'pval'], ascending=[False, True])\n",
        "\n",
        "    return df if return_df else df['gene'].head(n_genes).tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "185005af-e9be-4891-a95b-aea6489025a5",
      "metadata": {
        "id": "185005af-e9be-4891-a95b-aea6489025a5"
      },
      "source": [
        "#### get subscores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d134de59-e30c-469b-82b8-164a0825107c",
      "metadata": {
        "id": "d134de59-e30c-469b-82b8-164a0825107c"
      },
      "outputs": [],
      "source": [
        "def subscores(root_cell, adata,\n",
        "              markers_yaml='markers.yaml',\n",
        "              ontology_yaml='ontology.yaml'):\n",
        "    # init_ontology returns (G, markers)\n",
        "    G, _ = init_ontology(ontology_yaml, markers_yaml, root_cell)\n",
        "\n",
        "    # find the actual vertex for root_cell\n",
        "    root_v = G.vs.find(name=root_cell)\n",
        "\n",
        "    # now get all descendants (including the root itself)\n",
        "    desc_idxs = G.subcomponent(root_v.index, mode='OUT')\n",
        "    nodes = [G.vs[i]['name'] for i in desc_idxs]\n",
        "\n",
        "    existing, missing = [], []\n",
        "    for n in nodes:\n",
        "        col = f\"{n}_score\"\n",
        "        (existing if col in adata.obs else missing).append(col)\n",
        "\n",
        "    if missing:\n",
        "        print(f\"âš ï¸ Missing score columns: {missing}\")\n",
        "    return existing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c2725db-9a56-4a2c-9162-48ec2aa42133",
      "metadata": {
        "id": "5c2725db-9a56-4a2c-9162-48ec2aa42133"
      },
      "source": [
        "#### Remove non-coding genes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "73ede0f6-b7df-4573-95a3-4f27088e5036",
      "metadata": {
        "id": "73ede0f6-b7df-4573-95a3-4f27088e5036"
      },
      "outputs": [],
      "source": [
        "def coding_genes(adata, gene_id_type=None, chunk_size=1000):\n",
        "    \"\"\"\n",
        "    Filter an AnnData object to retain only protein-coding genes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adata : AnnData\n",
        "        Input object.\n",
        "    gene_id_type : str or None\n",
        "        'symbol' (e.g. 'CD3D') or 'ensembl.gene' (e.g. 'ENSG00000167286').\n",
        "        If None, it is guessed based on var_names.\n",
        "    chunk_size : int\n",
        "        Number of genes per MyGeneInfo query batch (max 1000).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    AnnData with only protein-coding genes in .X (and .raw, if present).\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # --- Guess gene ID type if not provided ---\n",
        "    if gene_id_type is None:\n",
        "        ens_like = sum(g.startswith(\"ENS\") for g in adata.var_names)\n",
        "        gene_id_type = \"ensembl.gene\" if ens_like / len(adata.var_names) > 0.5 else \"symbol\"\n",
        "    print(f\"ðŸ” Querying MyGeneInfo with scope: '{gene_id_type}'\")\n",
        "\n",
        "    gene_list = adata.var_names.tolist()\n",
        "    mg_client = mg.MyGeneInfo()\n",
        "\n",
        "    # --- Query in chunks ---\n",
        "    out = []\n",
        "    for i in range(0, len(gene_list), chunk_size):\n",
        "        chunk = gene_list[i:i + chunk_size]\n",
        "        try:\n",
        "            res = mg_client.querymany(\n",
        "                chunk,\n",
        "                scopes=gene_id_type,\n",
        "                fields='type_of_gene',\n",
        "                species='human',\n",
        "                as_dataframe=True,\n",
        "                df_index=True\n",
        "            )\n",
        "            out.append(res)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Query failed on chunk {i}: {e}\")\n",
        "\n",
        "    df = pd.concat(out)\n",
        "\n",
        "    # --- Keep clean hits only ---\n",
        "    df = df[~df['notfound'].fillna(False)]\n",
        "    if 'type_of_gene' not in df.columns:\n",
        "        raise ValueError(\"âŒ MyGeneInfo returned no 'type_of_gene' annotations.\")\n",
        "\n",
        "    # --- Map gene ID to biotype ---\n",
        "    biotype_map = df['type_of_gene'].to_dict()\n",
        "    adata_filtered = adata.copy()\n",
        "    adata_filtered.var['gene_biotype'] = adata_filtered.var_names.map(biotype_map)\n",
        "\n",
        "    coding_mask = adata_filtered.var['gene_biotype'] == 'protein-coding'\n",
        "    adata_filtered = adata_filtered[:, coding_mask].copy()\n",
        "\n",
        "    # --- Filter .raw if present ---\n",
        "    if adata.raw is not None and isinstance(adata.raw, ad.AnnData):\n",
        "        adata_filtered.raw = adata.raw[:, coding_mask].copy()\n",
        "\n",
        "    print(f\"âœ… Filtered from {adata.n_vars} to {adata_filtered.n_vars} protein-coding genes.\")\n",
        "    return adata_filtered\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OpOX6Q7oULt0",
      "metadata": {
        "id": "OpOX6Q7oULt0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6d0dc23b-f90c-4bf0-8c58-dfc8d8b50f84",
      "metadata": {
        "id": "6d0dc23b-f90c-4bf0-8c58-dfc8d8b50f84"
      },
      "source": [
        "#### QC computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d3f3f82b-798f-4453-8ffd-1deea5e6ebcd",
      "metadata": {
        "id": "d3f3f82b-798f-4453-8ffd-1deea5e6ebcd"
      },
      "outputs": [],
      "source": [
        "def compute_qc (adata):\n",
        "    if adata.raw is None:\n",
        "        raise ValueError('adata.raw is not set.')\n",
        "\n",
        "    X = adata.raw.X\n",
        "    gene_names = adata.raw.var_names\n",
        "\n",
        "    # Define gene masks\n",
        "    mt_mask = gene_names.str.startswith('MT-')\n",
        "    ribo_mask = gene_names.str.startswith(('RPS', 'RPL'))\n",
        "    hsp_mask = gene_names.str.startswith(('HSP', 'DNAJ'))\n",
        "\n",
        "    # Convert sparse to dense row sums if needed\n",
        "    row_sum = X.sum(axis=1).A1 if sp.issparse(X) else X.sum(axis=1)\n",
        "    gene_count = (X > 0).sum(axis=1).A1 if sp.issparse(X) else (X > 0).sum(axis=1)\n",
        "\n",
        "    mt_sum = X[:, mt_mask].sum(axis=1).A1 if sp.issparse(X) else X[:, mt_mask].sum(axis=1)\n",
        "    ribo_sum = X[:, ribo_mask].sum(axis=1).A1 if sp.issparse(X) else X[:, ribo_mask].sum(axis=1)\n",
        "    hsp_sum = X[:, hsp_mask].sum(axis=1).A1 if sp.issparse(X) else X[:, hsp_mask].sum(axis=1)\n",
        "\n",
        "    # Assign to .obs\n",
        "    adata.obs['total_counts'] = row_sum\n",
        "    adata.obs['n_genes_by_counts'] = gene_count\n",
        "    adata.obs['pct_counts_mt'] = (mt_sum / row_sum) * 100\n",
        "    adata.obs['pct_counts_ribo'] = (ribo_sum / row_sum) * 100\n",
        "    adata.obs['pct_counts_hsp'] = (hsp_sum / row_sum) * 100\n",
        "\n",
        "    return adata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0014c14-601e-41a0-9778-7b81fe10f78e",
      "metadata": {
        "id": "b0014c14-601e-41a0-9778-7b81fe10f78e"
      },
      "source": [
        "#### restore from raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0f60f046-b992-4429-b197-7cd605fcc926",
      "metadata": {
        "id": "0f60f046-b992-4429-b197-7cd605fcc926"
      },
      "outputs": [],
      "source": [
        "def restore_from_raw(adata):\n",
        "  # Step 1: Rebuild AnnData with full expression from .raw\n",
        "  adata_full = ad.AnnData(\n",
        "      X = adata.raw.X,\n",
        "      obs = adata.obs.copy(),\n",
        "      var = adata.raw.var.copy()\n",
        "  )\n",
        "  adata_full.var_names = adata.raw.var_names\n",
        "\n",
        "  # Step 2: Add embeddings and annotations\n",
        "  adata_full.obsm.update(adata.obsm)\n",
        "  adata_full.obsp.update(adata.obsp)\n",
        "  adata_full.uns.update(adata.uns)\n",
        "\n",
        "  # Step 3: Set .raw (now safe since .X and .var are full)\n",
        "  adata_full.raw = adata_full\n",
        "\n",
        "  # Step 4: Replace\n",
        "  adata = adata_full\n",
        "  print(f\"âœ… Full X and raw restored. shape = {adata.shape}, raw = {adata.raw.shape}\")\n",
        "  adata_full=[]\n",
        "  gc.collect()\n",
        "  return (adata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NhvYaj--0GpL",
      "metadata": {
        "id": "NhvYaj--0GpL"
      },
      "source": [
        "#### load 10x doublets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "wbI8oKzT0DsG",
      "metadata": {
        "id": "wbI8oKzT0DsG"
      },
      "outputs": [],
      "source": [
        "# load 10x doublets\n",
        "def load_10x_doublets (folder_path):\n",
        "    folder = Path(folder_path)\n",
        "    count_files = sorted(folder.glob('*count_matrix.tsv.gz'))\n",
        "    adata_list = []\n",
        "\n",
        "    for count_file in count_files:\n",
        "        # Infer base name and metadata file\n",
        "        base = re.sub(r'\\.count_matrix\\.tsv\\.gz$', '', count_file.name)\n",
        "        meta_file = folder / f'{base}.metadata.tsv.gz'\n",
        "        if not meta_file.exists():\n",
        "            print(f'Skipping {base}, no metadata file found.')\n",
        "            continue\n",
        "\n",
        "        # Load and orient counts\n",
        "        counts = pd.read_csv(count_file, sep='\\t', index_col=0)\n",
        "\n",
        "        # Load metadata\n",
        "        obs = pd.read_csv(meta_file, sep='\\t', index_col=0)\n",
        "\n",
        "        # Align\n",
        "        counts = counts.loc[obs.index]\n",
        "\n",
        "        # Harmonize metadata fields\n",
        "        donor_match = re.search(r'RH\\d+', base)\n",
        "        stim_match = re.search(r'(rest|stim)', base)\n",
        "\n",
        "        obs = obs.copy()\n",
        "        obs['donor'] = donor_match.group(0) if donor_match else 'unknown'\n",
        "        obs['in_vitro_stim'] = stim_match.group(1) if stim_match else 'unknown'\n",
        "        obs['condition'] = 'T1D'\n",
        "        obs['treatment'] = 'IL2'\n",
        "\n",
        "        if 'disease_state' not in obs.columns:\n",
        "            print(f\"Warning: 'disease_state' column missing in {base}\")\n",
        "\n",
        "        # Build AnnData\n",
        "        adata = sc.AnnData(X=counts.values, obs=obs, var=pd.DataFrame(index=counts.columns))\n",
        "        adata.obs['sample_id'] = base\n",
        "        adata_list.append(adata)\n",
        "\n",
        "    if adata_list:\n",
        "        return adata_list[0].concatenate(*adata_list[1:], batch_key='sample', index_unique=None)\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZq-lUo_i2If",
      "metadata": {
        "id": "BZq-lUo_i2If"
      },
      "source": [
        "#### Timecourse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "DFY_WniPmIkM",
      "metadata": {
        "id": "DFY_WniPmIkM"
      },
      "outputs": [],
      "source": [
        "def plot_timecourse(\n",
        "    adata,\n",
        "    features,\n",
        "    time_key='week',\n",
        "    patient_key='patient_id',\n",
        "    split_by=None,\n",
        "    color_by=None,\n",
        "    sem=True,\n",
        "    relative=False,\n",
        "    figsize=(10, 6),\n",
        "    ylim=(None, None),\n",
        "    ymin_zero=False,\n",
        "    palette=None  # NEW\n",
        "):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    for feature in features:\n",
        "        # Extract expression\n",
        "        if feature in adata.obs.columns:\n",
        "            vals = adata.obs[feature]\n",
        "        elif adata.raw is not None and feature in adata.raw.var_names:\n",
        "            arr = (\n",
        "                adata.raw[:, feature].X.toarray().ravel()\n",
        "                if sp.issparse(adata.raw.X)\n",
        "                else adata.raw[:, feature].X.ravel()\n",
        "            )\n",
        "            vals = pd.Series(arr, index=adata.obs_names, name=feature)\n",
        "        else:\n",
        "            raise KeyError(f\"Feature '{feature}' not found in obs or raw.\")\n",
        "\n",
        "        # Build base dataframe\n",
        "        df = pd.DataFrame({\n",
        "            time_key: pd.to_numeric(adata.obs[time_key], errors='coerce'),\n",
        "            patient_key: adata.obs[patient_key].astype(str),\n",
        "            feature: vals.values\n",
        "        }).dropna(subset=[time_key])\n",
        "        if split_by:\n",
        "            df[split_by] = adata.obs[split_by].astype(str)\n",
        "        if color_by:\n",
        "            df[color_by] = adata.obs[color_by].astype(str)\n",
        "\n",
        "        group_cols = [time_key, split_by] if split_by else [time_key]\n",
        "\n",
        "        # Step 1: Aggregate per patient\n",
        "        patient_means = (\n",
        "            df.groupby([time_key, patient_key] + ([split_by] if split_by else []), observed=True)[feature]\n",
        "              .mean()\n",
        "              .reset_index()\n",
        "        )\n",
        "\n",
        "        # Step 2: Compute stats across patients\n",
        "        stats = (\n",
        "            patient_means\n",
        "            .groupby(group_cols, observed=True)[feature]\n",
        "            .agg(mean='mean', std='std', count='count')\n",
        "            .reset_index()\n",
        "        )\n",
        "        stats['sem'] = stats['std'] / np.sqrt(stats['count'])\n",
        "\n",
        "        # Step 3: Assign colors\n",
        "        if split_by:\n",
        "            levels = stats[split_by].unique()\n",
        "            if palette is None:\n",
        "                colors = sns.color_palette(\"Set2\", len(levels))\n",
        "            elif isinstance(palette, str):\n",
        "                try:\n",
        "                    colors = sns.color_palette(palette, len(levels))\n",
        "                except:\n",
        "                    cmap = plt.get_cmap(palette)\n",
        "                    colors = cmap(np.linspace(0.2, 0.9, len(levels)))\n",
        "            elif callable(palette):  # colormap\n",
        "                colors = palette(np.linspace(0.2, 0.9, len(levels)))\n",
        "            else:  # list\n",
        "                if len(palette) < len(levels):\n",
        "                    raise ValueError(\"Palette too short for number of groups.\")\n",
        "                colors = palette\n",
        "            color_dict = dict(zip(levels, colors))\n",
        "\n",
        "        # Relative expression (% of baseline)\n",
        "        if relative:\n",
        "            if split_by:\n",
        "                stats['mean'] = stats.groupby(split_by)['mean'].transform(\n",
        "                    lambda x: (x / x.iloc[0]) * 100 if x.iloc[0] > 0 else 0\n",
        "                )\n",
        "                stats['sem'] = stats.groupby(split_by)['sem'].transform(\n",
        "                    lambda x: (x / x.iloc[0]) * 100 if x.iloc[0] > 0 else 0\n",
        "                )\n",
        "            else:\n",
        "                base = stats['mean'].iloc[0]\n",
        "                stats['mean'] = (stats['mean'] / base) * 100 if base > 0 else 0\n",
        "                stats['sem'] = (stats['sem'] / base) * 100 if base > 0 else 0\n",
        "            y_label = 'Relative mean expression (%)'\n",
        "        else:\n",
        "            y_label = \"Mean Â± SEM\" if sem else \"Mean\"\n",
        "\n",
        "        # Plotting\n",
        "        if split_by:\n",
        "            for name, subdf in stats.groupby(split_by, observed=True):\n",
        "                subdf = subdf.sort_values(by=time_key)\n",
        "                ax.errorbar(\n",
        "                    subdf[time_key],\n",
        "                    subdf['mean'],\n",
        "                    yerr=subdf['sem'] if sem else None,\n",
        "                    marker='o',\n",
        "                    linestyle='-',\n",
        "                    label=name,\n",
        "                    linewidth=1,\n",
        "                    color=color_dict[name],\n",
        "                    capsize=4\n",
        "                )\n",
        "        else:\n",
        "            stats = stats.sort_values(by=time_key)\n",
        "            ax.errorbar(\n",
        "                stats[time_key],\n",
        "                stats['mean'],\n",
        "                yerr=stats['sem'] if sem else None,\n",
        "                marker='o',\n",
        "                linestyle='-',\n",
        "                label=feature,\n",
        "                linewidth=1,\n",
        "                capsize=4\n",
        "            )\n",
        "\n",
        "    # Legend\n",
        "    legend_title = split_by.capitalize() if split_by else \"Feature\"\n",
        "    ax.legend(title=legend_title, loc=None) #, bbox_to_anchor=(1.02, 1))\n",
        "\n",
        "    # Labels and styling\n",
        "    ax.set_xlabel(time_key)\n",
        "    ax.set_ylabel(y_label)\n",
        "    title = f\"Time course of {features[0]}\" if len(features) == 1 else \"Time course of features\"\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Y-axis limits\n",
        "    if ymin_zero:\n",
        "        top = ylim[1] if isinstance(ylim, (tuple, list)) and len(ylim) == 2 else None\n",
        "        ax.set_ylim(bottom=0, top=top)\n",
        "    elif isinstance(ylim, (tuple, list)) and any(v is not None for v in ylim):\n",
        "        ax.set_ylim(ylim)\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SgR5lzIDZHPm",
      "metadata": {
        "id": "SgR5lzIDZHPm"
      },
      "source": [
        "#### Ontology & Marker Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "iR3NkNDCZEcX",
      "metadata": {
        "id": "iR3NkNDCZEcX"
      },
      "outputs": [],
      "source": [
        "def init_ontology(ontology_yaml: str, markers_yaml: str, root: str):\n",
        "    \"\"\"\n",
        "    Load ontology and marker definitions.\n",
        "\n",
        "    Returns:\n",
        "        - G: an igraph.Graph representing the cell ontology hierarchy, used for recursive annotation and optional visualization.\n",
        "        - normalized: a dictionary of markers per node (with 'pos' and 'neg' lists), used for scoring and pruning.\n",
        "\n",
        "    The YAML files provide the editable source-of-truth; the graph and marker dict serve distinct internal purposes.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Load ontology and marker definitions, return igraph and normalized marker dict.\n",
        "    \"\"\"\n",
        "    with open(ontology_yaml, 'r') as f:\n",
        "        ont = yaml.safe_load(f)\n",
        "    with open(markers_yaml, 'r') as f:\n",
        "        raw = yaml.safe_load(f)\n",
        "    raw_markers = raw.get('markers', raw)\n",
        "\n",
        "    # Build directed tree\n",
        "    G = ig.Graph(directed=True)\n",
        "    G.add_vertex(root)\n",
        "\n",
        "    def find_subtree(d):\n",
        "        if not isinstance(d, dict):\n",
        "            return None\n",
        "        if root in d:\n",
        "            return d[root]\n",
        "        for v in d.values():\n",
        "            sub = find_subtree(v)\n",
        "            if sub is not None:\n",
        "                return sub\n",
        "        return None\n",
        "\n",
        "    subtree = find_subtree(ont)\n",
        "    if subtree is None:\n",
        "        raise KeyError(f\"Root '{root}' not found in ontology '{ontology_yaml}'\")\n",
        "\n",
        "    def recurse(parent, branch):\n",
        "        if not isinstance(branch, dict):\n",
        "            return\n",
        "        for child, grand in branch.items():\n",
        "            if child.startswith('!'):\n",
        "                continue\n",
        "            if child not in G.vs['name']:\n",
        "                G.add_vertex(child)\n",
        "            G.add_edge(parent, child)\n",
        "            recurse(child, grand or {})\n",
        "\n",
        "    recurse(root, subtree)\n",
        "\n",
        "    # Normalize markers\n",
        "    normalized = {}\n",
        "    for node, defs in raw_markers.items():\n",
        "        if isinstance(defs, dict):\n",
        "            pos = defs.get('pos', [])\n",
        "            neg = defs.get('neg', [])\n",
        "            if isinstance(pos, str): pos = [pos]\n",
        "            if isinstance(neg, str): neg = [neg]\n",
        "        elif isinstance(defs, list):\n",
        "            pos = [g for g in defs if not g.startswith('-')]\n",
        "            neg = [g.lstrip('-') for g in defs if g.startswith('-')]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid markers for {node!r}: {defs!r}\")\n",
        "        normalized[node] = {'pos': pos, 'neg': neg}\n",
        "\n",
        "    # Validate completeness\n",
        "    subtree_indices = set(G.subcomponent(root, mode='OUT')) | {G.vs.find(name=root).index}\n",
        "    subtree_names = {G.vs[i]['name'] for i in subtree_indices}\n",
        "    missing = subtree_names - set(normalized)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing marker definitions for: {sorted(missing)}\")\n",
        "\n",
        "    # Attach marker lists as vertex attributes\n",
        "    for v in G.vs:\n",
        "        name = v['name']\n",
        "        v['pos_markers'] = normalized[name]['pos']\n",
        "        v['neg_markers'] = normalized[name]['neg']\n",
        "\n",
        "    return G, normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FZRPpM0BmYzL",
      "metadata": {
        "id": "FZRPpM0BmYzL"
      },
      "source": [
        "#### Sanitize sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7v4pdFBamXBR",
      "metadata": {
        "id": "7v4pdFBamXBR"
      },
      "outputs": [],
      "source": [
        "def sanitize_sparse(adata):\n",
        "    if sp.issparse(adata.X) and not isinstance(adata.X, sp.csr_matrix):\n",
        "        adata.X = adata.X.tocsr()\n",
        "    for k in adata.layers.keys():\n",
        "        if sp.issparse(adata.layers[k]) and not isinstance(adata.layers[k], sp.csr_matrix):\n",
        "            adata.layers[k] = adata.layers[k].tocsr()\n",
        "    for k in adata.obsp.keys():\n",
        "        if sp.issparse(adata.obsp[k]) and not isinstance(adata.obsp[k], sp.csr_matrix):\n",
        "            adata.obsp[k] = adata.obsp[k].tocsr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e7165a-610d-4d89-baeb-5fd8bf65235b",
      "metadata": {
        "id": "39e7165a-610d-4d89-baeb-5fd8bf65235b"
      },
      "source": [
        "#### SC pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "FEAa9v2VQvfD",
      "metadata": {
        "id": "FEAa9v2VQvfD"
      },
      "outputs": [],
      "source": [
        "def pipeline(\n",
        "    adata,\n",
        "    sample_key='sample',\n",
        "    n_top_genes=2000,\n",
        "    exclude_xcr=True,\n",
        "    harmony=True,\n",
        "    convert_aliases=False,\n",
        "    umap_min_dist=0.5,\n",
        "    umap_spread=1.0,\n",
        "    leiden_resolution=1.0,\n",
        "    downsample_n: int | None = None\n",
        "):\n",
        "    print(\"ðŸ”„ Starting pipelineâ€¦\")\n",
        "    adata = convert (adata)\n",
        "    # Step 1: Downsampling\n",
        "    if downsample_n is not None and downsample_n < adata.n_obs:\n",
        "        print(f\"ðŸ“‰ Step 1: Downsampling to {downsample_n} cellsâ€¦\")\n",
        "        idx = np.random.choice(adata.n_obs, downsample_n, replace=False)\n",
        "        adata_sub = adata[idx].copy()\n",
        "    else:\n",
        "        print(\"âž¡ï¸ Step 1: No downsampling.\")\n",
        "        adata_sub = adata\n",
        "\n",
        "    # Step 2: Remove zero-sum genes\n",
        "    print(\"ðŸ§¹ Step 2: Filtering zeroâ€sum genesâ€¦\")\n",
        "    sums = np.ravel(adata.X.sum(axis=0)) if sp.issparse(adata.X) else adata.X.sum(axis=0)\n",
        "    mask = (sums > 0) & (~np.isnan(sums))\n",
        "    before = adata.n_vars\n",
        "    adata = adata[:, mask].copy()\n",
        "    adata_sub = adata_sub[:, mask].copy()\n",
        "    gc.collect()\n",
        "    print(f\"   â†’ {before} â†’ {adata.n_vars} genes kept.\")\n",
        "\n",
        "    # Step 3: Optional gene alias conversion\n",
        "    if convert_aliases:\n",
        "        print(\"ðŸ” Step 3: Converting gene aliasesâ€¦\")\n",
        "        adata = convert(adata)\n",
        "        adata_sub = convert(adata_sub)\n",
        "    else:\n",
        "        print(\"â­ï¸ Step 3: Skipping alias conversion.\")\n",
        "\n",
        "    # Step 4: Normalize + log1p\n",
        "    print(\"Step 4: Normalizing & log1pâ€¦\")\n",
        "    for ds, name in ((adata, 'full'), (adata_sub, 'subset')):\n",
        "        if ds.X.max() > 100:\n",
        "            sc.pp.normalize_total(ds, target_sum=1e4)\n",
        "            sc.pp.log1p(ds)\n",
        "            print(f\"   - {name} normalized\")\n",
        "        else:\n",
        "            print(f\"   - {name} already transformed\")\n",
        "\n",
        "    # Step 4b: Cache normalized+log1p as raw\n",
        "    cached_raw = adata.copy()\n",
        "    cached_raw.raw = None\n",
        "    adata.raw = cached_raw\n",
        "    print(\"ðŸ’¾ Step 4b: Cached normalized+log1p data in adata.raw.\")\n",
        "\n",
        "    # Step 5: HVGs\n",
        "    print(\"âœ¨ Step 5: Selecting HVGsâ€¦\")\n",
        "    sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_top_genes, flavor='seurat_v3', inplace=True)\n",
        "    hvgs = adata_sub.var_names[adata_sub.var['highly_variable']].tolist()\n",
        "\n",
        "    if exclude_xcr:\n",
        "        bcr = adata.var_names.str.match(r'^IG[HKL]V')\n",
        "        tcr = adata.var_names.str.match(r'^TR[ABGD]V')\n",
        "        hvgs = [g for g in hvgs if not (bcr[adata.var_names.get_loc(g)] or tcr[adata.var_names.get_loc(g)])]\n",
        "        print(f\"   â†’ Excluding BCR/TCR; {len(hvgs)} HVGs remain.\")\n",
        "    else:\n",
        "        print(f\"   â†’ {len(hvgs)} HVGs selected.\")\n",
        "\n",
        "    adata = adata[:, hvgs].copy()\n",
        "    adata_sub = adata_sub[:, hvgs].copy()\n",
        "\n",
        "    # Step 6: PCA on subset, projection to full\n",
        "    print(\"ðŸ§® Step 6: Scaling + PCAâ€¦\")\n",
        "    sc.pp.scale(adata_sub, zero_center=True, max_value=10)\n",
        "    sc.tl.pca(adata_sub, svd_solver='arpack', n_comps=50)\n",
        "    print(\"   â†’ PCA on subset complete.\")\n",
        "\n",
        "    # âš ï¸ Fix: scale full data before projection\n",
        "    sc.pp.scale(adata, zero_center=True, max_value=10)\n",
        "    adata.obsm['X_pca'] = adata.X @ adata_sub.varm['PCs']\n",
        "    print(\"   â†’ Projected full dataset into PCA space.\")\n",
        "\n",
        "    # Step 7: Harmony (optional)\n",
        "    rep = 'X_pca'\n",
        "    if harmony:\n",
        "        if sample_key not in adata.obs.columns:\n",
        "            raise KeyError(f\"Cannot run Harmony: '{sample_key}' not in adata.obs\")\n",
        "\n",
        "        print(f\"ðŸ”€ Step 7: Running Harmony on '{sample_key}'â€¦\")\n",
        "\n",
        "        if sp.issparse(adata.obsm['X_pca']):\n",
        "            adata.obsm['X_pca'] = adata.obsm['X_pca'].toarray()\n",
        "\n",
        "        nan_mask = np.isnan(adata.obsm['X_pca']).any(axis=1)\n",
        "        if nan_mask.any():\n",
        "            print(f\"   âš ï¸ Found {nan_mask.sum()} cells with NaNs â€” removing them before Harmony.\")\n",
        "            adata = adata[~nan_mask].copy()\n",
        "\n",
        "        try:\n",
        "            import harmonypy as hm\n",
        "            ho = hm.run_harmony(adata.obsm['X_pca'], adata.obs, sample_key)\n",
        "            adata.obsm['X_pca_harmony'] = ho.Z_corr.T\n",
        "            rep = 'X_pca_harmony'\n",
        "            print(\"   â†’ Harmony integration complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"   âš ï¸ Harmony failed ({e}); using PCA instead.\")\n",
        "\n",
        "    # Step 8â€“10: neighbors, UMAP, clustering\n",
        "    sc.pp.neighbors(adata, use_rep=rep, n_pcs=50)\n",
        "    print(\"ðŸ§­ Step 8: Neighbors calculated.\")\n",
        "\n",
        "    sc.tl.umap(adata, init_pos='random', min_dist=umap_min_dist, spread=umap_spread)\n",
        "    print(\"ðŸ—ºï¸ Step 9: UMAP computed.\")\n",
        "\n",
        "    sc.tl.leiden(adata, resolution=leiden_resolution, key_added='leiden_1', flavor=\"igraph\", n_iterations=2)\n",
        "    print(\"ðŸ§¬ Step 10: Leiden clustering complete.\")\n",
        "\n",
        "    # Step 11â€“12: Restore raw and cleanup\n",
        "    adata.raw = cached_raw\n",
        "    print(\"â™»ï¸ Step 11: Restored adata.raw.\")\n",
        "\n",
        "    def sanitize_sparse(a):\n",
        "        if sp.issparse(a.X):\n",
        "            a.X.sort_indices()\n",
        "\n",
        "    print(\"ðŸ§¼ Step 12: Sanitizing sparse matrixâ€¦\")\n",
        "    sanitize_sparse(adata)\n",
        "\n",
        "    print(\"âœ… Pipeline complete.\")\n",
        "    return adata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70176226-988a-423a-a2f9-edc790450c55",
      "metadata": {
        "id": "70176226-988a-423a-a2f9-edc790450c55"
      },
      "source": [
        "#### FastTag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "35a0a4cb-5427-4907-92d6-53a145c3d6f6",
      "metadata": {
        "id": "35a0a4cb-5427-4907-92d6-53a145c3d6f6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def FastTag(\n",
        "    adata,\n",
        "    ontology_yaml: str,\n",
        "    markers_yaml: str,\n",
        "    root: str,\n",
        "    smoothing_cycles: int = 1,\n",
        "    majority_vote: bool = False,\n",
        "    min_marker_count: int = 2,\n",
        "    pruning_thresholds: dict = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Tag cells based on marker ontology using recursive splitting via igraph.\n",
        "    Requires neighbors and normalized_connectivities (from pipeline()).\n",
        "    \"\"\"\n",
        "    # Ensure neighbors and connectivities exist\n",
        "    if 'neighbors' not in adata.uns or 'connectivities' not in adata.obsp:\n",
        "        raise RuntimeError(\n",
        "            \"Missing neighbors/connectivities. Please run pipeline() before FastTag.\"\n",
        "        )\n",
        "\n",
        "    if 'normalized_connectivities' not in adata.obsp:\n",
        "        adata.obsp['normalized_connectivities'] = adata.obsp['connectivities']\n",
        "\n",
        "    # Set default thresholds if not provided\n",
        "    if pruning_thresholds is None:\n",
        "        pruning_thresholds = {\n",
        "            'max_expr': 0.5,\n",
        "            'std_expr': 0.15,\n",
        "            'min_corr': 0\n",
        "        }\n",
        "\n",
        "    # Load ontology graph and normalized markers\n",
        "    G, original_markers = init_ontology(ontology_yaml, markers_yaml, root)\n",
        "    root_vertex = G.vs.find(name=root)\n",
        "    subgraph_nodes = G.subcomponent(root_vertex, mode='OUT')\n",
        "    subgraph_names = [G.vs[i]['name'] for i in subgraph_nodes]\n",
        "    subgraph_markers = {k: v for k, v in original_markers.items() if k in subgraph_names}\n",
        "\n",
        "    # Score cells with full marker set (for pruning)\n",
        "    full_scores = score_cells(adata, subgraph_markers, smoothing_cycles)\n",
        "\n",
        "    # Prune markers based on expression patterns and full scores\n",
        "    pruned_markers = prune_markers(\n",
        "        adata=adata,\n",
        "        marker_definitions=subgraph_markers,\n",
        "        min_marker_count=min_marker_count,\n",
        "        thresholds=pruning_thresholds,\n",
        "        full_scores=full_scores\n",
        "    )\n",
        "\n",
        "    # Score cells with pruned marker set\n",
        "    scores = score_cells(adata, pruned_markers, smoothing_cycles)\n",
        "\n",
        "    # Recursive splitting\n",
        "    labels = pd.Series(index=scores.index, dtype=object)\n",
        "\n",
        "    def assign(node, cell_idx):\n",
        "        children = [G.vs[i]['name'] for i in G.successors(G.vs.find(name=node)) if G.vs[i]['name'] in scores.columns.str.replace('_score', '')]\n",
        "        if not children:\n",
        "            labels.iloc[cell_idx] = node\n",
        "            return\n",
        "        cols = [f\"{c}_score\" for c in children if f\"{c}_score\" in scores.columns]\n",
        "        sub = scores.iloc[cell_idx][cols].values\n",
        "        best = sub.argmax(axis=1)\n",
        "        pos = sub.max(axis=1) > 0\n",
        "        for j, child in enumerate(children):\n",
        "            idxs = cell_idx[pos & (best == j)]\n",
        "            if idxs.size:\n",
        "                assign(child, idxs)\n",
        "        rem = cell_idx[~pos]\n",
        "        if rem.size:\n",
        "            labels.iloc[rem] = node\n",
        "\n",
        "    assign(root, np.arange(scores.shape[0]))\n",
        "    adata.obs['FastTag'] = pd.Categorical(labels.values, categories=subgraph_names)\n",
        "\n",
        "    # Optional majority vote smoothing\n",
        "    if majority_vote:\n",
        "        conn = adata.obsp['normalized_connectivities']\n",
        "        oh = pd.get_dummies(adata.obs['FastTag']).values\n",
        "        votes = conn.dot(oh)\n",
        "        idx = votes.argmax(axis=1)\n",
        "        cats = pd.get_dummies(adata.obs['FastTag']).columns\n",
        "        sm = [cats[i] for i in idx]\n",
        "        adata.obs['FastTag'] = pd.Categorical(sm, categories=cats)\n",
        "\n",
        "    # Store score columns\n",
        "    adata.obs['FastTag'] = adata.obs['FastTag'].cat.remove_unused_categories()\n",
        "    for col in scores.columns:\n",
        "        adata.obs[col] = scores[col]\n",
        "\n",
        "    return adata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CIRxHDF5T8z3",
      "metadata": {
        "id": "CIRxHDF5T8z3"
      },
      "source": [
        "#### Prune markers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "etxuufCBT54m",
      "metadata": {
        "id": "etxuufCBT54m"
      },
      "outputs": [],
      "source": [
        "def prune_markers(\n",
        "    adata,\n",
        "    marker_definitions,\n",
        "    min_marker_count=2,\n",
        "    thresholds=None,\n",
        "    full_scores=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Prune uninformative markers per ontology node based on max expression,\n",
        "    variability, and correlation with smoothed cell scores.\n",
        "    Uses raw data for evaluation.\n",
        "    \"\"\"\n",
        "    pruned_markers = {}\n",
        "\n",
        "    for node, markers in marker_definitions.items():\n",
        "        pos = [g for g in markers.get('pos', []) if g in adata.raw.var_names]\n",
        "        neg = [g for g in markers.get('neg', []) if g in adata.raw.var_names]\n",
        "\n",
        "        pos_idx = [adata.raw.var_names.get_loc(g) for g in pos]\n",
        "        neg_idx = [adata.raw.var_names.get_loc(g) for g in neg]\n",
        "\n",
        "        X_pos = adata.raw.X[:, pos_idx]\n",
        "        X_neg = adata.raw.X[:, neg_idx] if neg_idx else None\n",
        "\n",
        "        if sp.issparse(X_pos):\n",
        "            X_pos = X_pos.toarray()\n",
        "        X_pos = np.expm1(X_pos)\n",
        "\n",
        "        if X_neg is not None and sp.issparse(X_neg):\n",
        "            X_neg = X_neg.toarray()\n",
        "        if X_neg is not None:\n",
        "            X_neg = np.expm1(X_neg)\n",
        "\n",
        "        max_vals_pos = X_pos.max(axis=0)\n",
        "        std_vals_pos = X_pos.std(axis=0)\n",
        "        max_vals_neg = X_neg.max(axis=0) if X_neg is not None else []\n",
        "        std_vals_neg = X_neg.std(axis=0) if X_neg is not None else []\n",
        "\n",
        "        keep_pos, keep_neg = [], []\n",
        "\n",
        "        for i, gene in enumerate(pos):\n",
        "            reasons = []\n",
        "            if thresholds and max_vals_pos[i] < thresholds['max_expr']:\n",
        "                reasons.append(f\"low max ({max_vals_pos[i]:.2f})\")\n",
        "            if thresholds and std_vals_pos[i] < thresholds['std_expr']:\n",
        "                reasons.append(f\"low std ({std_vals_pos[i]:.2f})\")\n",
        "            if full_scores is not None and thresholds:\n",
        "                try:\n",
        "                    score_vec = full_scores[f\"{node}_score\"].values\n",
        "                    r, _ = pearsonr(X_pos[:, i].flatten(), score_vec)\n",
        "                    if r < thresholds['min_corr']:\n",
        "                        reasons.append(f\"low corr ({r:.2f})\")\n",
        "                except Exception:\n",
        "                    reasons.append(\"correlation error\")\n",
        "            if reasons:\n",
        "                print(f\"[FastTag:prune] Excluding '{gene}' from '{node}': {', '.join(reasons)}\")\n",
        "            else:\n",
        "                keep_pos.append(gene)\n",
        "\n",
        "        for i, gene in enumerate(neg):\n",
        "            reasons = []\n",
        "            if thresholds and max_vals_neg[i] < thresholds['max_expr']:\n",
        "                reasons.append(f\"low max ({max_vals_neg[i]:.2f})\")\n",
        "            if thresholds and std_vals_neg[i] < thresholds['std_expr']:\n",
        "                reasons.append(f\"low std ({std_vals_neg[i]:.2f})\")\n",
        "            if full_scores is not None and thresholds:\n",
        "                try:\n",
        "                    score_vec = full_scores[f\"{node}_score\"].values\n",
        "                    r, _ = pearsonr(X_neg[:, i].flatten(), score_vec)\n",
        "                    if abs(r) < thresholds['min_corr'] or r > 0:\n",
        "                        reasons.append(f\"low or wrong-sign corr ({r:.2f})\")\n",
        "                except Exception:\n",
        "                    reasons.append(\"correlation error\")\n",
        "            if reasons:\n",
        "                print(f\"[FastTag:prune] Excluding '{gene}' from '{node}': {', '.join(reasons)}\")\n",
        "            else:\n",
        "                keep_neg.append(gene)\n",
        "\n",
        "        if len(keep_pos) + len(keep_neg) < min_marker_count:\n",
        "            print(f\"[FastTag:prune] Node '{node}' skipped: only {len(keep_pos) + len(keep_neg)} markers passed filters (min required = {min_marker_count}).\")\n",
        "            pruned_markers[node] = {'pos': [], 'neg': []}\n",
        "        else:\n",
        "            pruned_markers[node] = {'pos': keep_pos, 'neg': keep_neg}\n",
        "\n",
        "    return pruned_markers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aheq4n-VR76v",
      "metadata": {
        "id": "aheq4n-VR76v"
      },
      "source": [
        "#### Tag cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "x0J3GZWQR66D",
      "metadata": {
        "id": "x0J3GZWQR66D"
      },
      "outputs": [],
      "source": [
        "def tag_cells(scores: pd.DataFrame, nodes: list) -> pd.Categorical:\n",
        "    idx = np.nanargmax(scores.values, axis=1)\n",
        "    labels = [scores.columns[i].replace('_score', '') for i in idx]\n",
        "    return pd.Categorical(labels, categories=nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GLLiQVa5Rwk4",
      "metadata": {
        "id": "GLLiQVa5Rwk4"
      },
      "source": [
        "#### Smooth expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "jazOuEbaRu6P",
      "metadata": {
        "id": "jazOuEbaRu6P"
      },
      "outputs": [],
      "source": [
        "def smooth_expr(adata, genes):\n",
        "    \"\"\"\n",
        "    Smooth expression using precomputed connectivity.\n",
        "    \"\"\"\n",
        "    conn = adata.obsp['normalized_connectivities']\n",
        "    idx = [adata.raw.var_names.get_loc(g) for g in genes]\n",
        "    X = adata.raw.X[:, idx]\n",
        "    if sp.issparse(X):\n",
        "        X = X.toarray()\n",
        "    X = np.expm1(X)\n",
        "    mat = conn.dot(X)\n",
        "    mat = np.log1p(mat)\n",
        "    return pd.DataFrame(mat, index=adata.obs_names, columns=genes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FQzQ2nETRYDW",
      "metadata": {
        "id": "FQzQ2nETRYDW"
      },
      "source": [
        "#### Score cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "sedK-3jKRT7i",
      "metadata": {
        "id": "sedK-3jKRT7i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "\n",
        "def score_cells(adata, markers, smoothing_cycles=1, min_genes=1):\n",
        "    \"\"\"\n",
        "    Compute robust, normalized per-cell scores for each node based on marker gene expression.\n",
        "\n",
        "    Parameters:\n",
        "        adata : AnnData object (must have .raw with log1p-transformed counts)\n",
        "        markers : dict of node -> {'pos': [...], 'neg': [...]}\n",
        "        smoothing_cycles : int, optional (default=1), number of smoothing iterations\n",
        "        min_genes : int, minimum number of positive markers required for scoring\n",
        "\n",
        "    Returns:\n",
        "        DataFrame of shape (n_cells, n_nodes) with scores in [0, 1]\n",
        "    \"\"\"\n",
        "    # Collect all marker genes\n",
        "    all_genes = sorted({g for defs in markers.values() for g in defs['pos'] + defs['neg']})\n",
        "    found = [g for g in all_genes if g in adata.raw.var_names]\n",
        "    if not found:\n",
        "        raise ValueError(\"No marker genes found in adata.raw\")\n",
        "\n",
        "    # Extract raw expression matrix\n",
        "    idx = [adata.raw.var_names.get_loc(g) for g in found]\n",
        "    X = adata.raw.X[:, idx]\n",
        "    if sp.issparse(X):\n",
        "        X = X.toarray()\n",
        "    X = np.expm1(X)  # reverse log1p\n",
        "    expr = pd.DataFrame(X, index=adata.obs_names, columns=found)\n",
        "\n",
        "    # Optional smoothing\n",
        "    for _ in range(smoothing_cycles):\n",
        "        expr = smooth_expr(adata, expr.columns.tolist())  # assumes smooth_expr returns a DataFrame\n",
        "\n",
        "    # Score calculation\n",
        "    score_frames = []\n",
        "    for node, defs in markers.items():\n",
        "        pos = [g for g in defs['pos'] if g in expr]\n",
        "        neg = [g for g in defs['neg'] if g in expr]\n",
        "        if len(pos) < min_genes:\n",
        "            continue\n",
        "\n",
        "        diff = expr[pos].mean(axis=1) - expr[neg].mean(axis=1).fillna(0)\n",
        "        diff = diff.clip(lower=0)\n",
        "\n",
        "        # Normalize using 1st and 99th percentiles\n",
        "        q1, q99 = np.percentile(diff, [1, 99])\n",
        "        norm_score = (diff - q1) / (q99 - q1 + 1e-9)\n",
        "        norm_score = norm_score.clip(0, 1)\n",
        "\n",
        "        score_frames.append(norm_score.rename(f\"{node}_score\"))\n",
        "\n",
        "    return pd.concat(score_frames, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "veAlkrPm24F4",
      "metadata": {
        "id": "veAlkrPm24F4"
      },
      "source": [
        "#### Ontology Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "B26WcKmP2yhW",
      "metadata": {
        "id": "B26WcKmP2yhW"
      },
      "outputs": [],
      "source": [
        "# === Lineage colors ===\n",
        "lineage_to_color = {\n",
        "    'CD4': '#B3E2B2',\n",
        "    'CD8': '#A6D9D5',\n",
        "    'ILC': '#BFD3E6',\n",
        "    'B': '#D5DDF0',\n",
        "    'myelo': '#E2E2E2',\n",
        "    'shared': '#C4CDD6'}\n",
        "\n",
        "# === Cell types grouped by lineage ===\n",
        "lineage_to_cells = {\n",
        "    'CD4': [\n",
        "        'CD4', 'c_CD4', 'mem_CD4', 'cm_CD4', 'CCR7_CD4',\n",
        "        'naive_CD4', 'Th1', 'Th17', 'Th2', 'Tfh',\n",
        "        'Treg', 'naive_Treg', 'e_Treg', 'k_Treg'\n",
        "    ],\n",
        "    'CD8': [\n",
        "        'CD8', 'mem_CD8', 'naive_CD8', 'cm_CD8',\n",
        "        'em_CD8', 'tem_CD8',  'MAIT'\n",
        "    ],\n",
        "    'ILC': [\n",
        "        'ILC', 'ILC3', 'CD16_NK', 'CD56_NK', 'T_NK'\n",
        "    ],\n",
        "    'B': [\n",
        "        'B', 'CD19_B', 'mem_B', 'naive_B',\n",
        "        'ABC', 'circ_B', 'plasma_blast', 'plasma'\n",
        "    ],\n",
        "    'myelo': [\n",
        "        'myelo', 'MPC', 'mono', 'c_mono', 'nc_mono',\n",
        "        'int_mono', 'cDC1', 'cDC2', 'DC5', 'pDC'\n",
        "    ],\n",
        "    'shared': [\n",
        "        'PBMC', 'T', 'T_NK' , 'ab_T',  'gd_T'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# === Helper to turn lineage tables into a color lookup table (palette) ===\n",
        "def make_palette(lineage_to_cells, lineage_to_color):\n",
        "    return {\n",
        "        cell: lineage_to_color[lineage]\n",
        "        for lineage, cells in lineage_to_cells.items()\n",
        "        for cell in cells\n",
        "    }\n",
        "\n",
        "# === Ontology tree plotting function ===\n",
        "def plot_ontology(\n",
        "    G,\n",
        "    root=None,\n",
        "    vertex_size=20,\n",
        "    vertex_label_size=10,\n",
        "    bbox=(800,800),\n",
        "    margin=50,\n",
        "    palette=None,\n",
        "    wrap_width=6\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot a cell ontology tree using igraph.\n",
        "    - Colors are assigned using a palette generated by make_palette.\n",
        "    - Accepts a dict mapping cell type names to colors.\n",
        "    - Supports automatic layout from root.\n",
        "    \"\"\"\n",
        "    # Prepare labels\n",
        "    names = G.vs['name']\n",
        "    wrapped = [textwrap.fill(n.replace('_',' '), width=wrap_width, break_long_words=True) for n in names]\n",
        "\n",
        "    # Layout\n",
        "    if root:\n",
        "        ridx = G.vs.find(name=root).index\n",
        "        layout = G.layout_reingold_tilford(root=[ridx], mode='out')\n",
        "    else:\n",
        "        layout = G.layout_reingold_tilford(mode='out')\n",
        "    coords = [(x,-y) for x,y in layout]\n",
        "\n",
        "    # Colors\n",
        "    if palette is None:\n",
        "        cols = ['skyblue']*len(G.vs)\n",
        "    elif isinstance(palette, dict):\n",
        "        cols = [palette[n] for n in names]\n",
        "    else:\n",
        "        raise ValueError('palette must be a dict or None')\n",
        "\n",
        "    G.vs['color'] = cols\n",
        "\n",
        "    # Plot\n",
        "    dpi = plt.rcParams['figure.dpi']\n",
        "    fig, ax = plt.subplots(figsize=(bbox[0]/dpi, bbox[1]/dpi))\n",
        "    ax.set_axis_off()\n",
        "    ig.plot(\n",
        "      G.as_undirected(),\n",
        "      target=ax,\n",
        "      layout=coords,\n",
        "      vertex_label=wrapped,\n",
        "      vertex_size=vertex_size,\n",
        "      vertex_label_size=vertex_label_size,\n",
        "      margin=margin,\n",
        "      edge_color=\"gray\",\n",
        "      vertex_frame_color=\"dimgray\"\n",
        "      )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qicvXPpxSvNj",
      "metadata": {
        "id": "qicvXPpxSvNj"
      },
      "source": [
        "### Download YAMLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3weAPKtZS0kx",
      "metadata": {
        "id": "3weAPKtZS0kx"
      },
      "outputs": [],
      "source": [
        "fetch(['ontology.yaml','ontology_TIL.yaml','markers.yaml','coarse.yaml', 'notes.txt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bfbe190-f46c-4f82-8d55-c646011dc161",
      "metadata": {
        "id": "5bfbe190-f46c-4f82-8d55-c646011dc161"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F3t5eL4SesKs",
      "metadata": {
        "id": "F3t5eL4SesKs"
      },
      "source": [
        "### Load from gcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vuGZPpwOepyA",
      "metadata": {
        "id": "vuGZPpwOepyA"
      },
      "outputs": [],
      "source": [
        "gc.collect ()\n",
        "fetch (\"FL/FL.h5ad\")\n",
        "adata = sc.read_h5ad ('/content/FL.h5ad')\n",
        "G, normalized_markers = init_ontology('coarse.yaml', 'markers.yaml', root='root')\n",
        "globals()['normalized_markers'] = normalized_markers\n",
        "globals()['G'] = G\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PVYrk-ANdCX-",
      "metadata": {
        "id": "PVYrk-ANdCX-"
      },
      "source": [
        "### Load .h5ad file from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9101fdf-09ca-4d93-969c-d72f7a2a1990",
      "metadata": {
        "id": "b9101fdf-09ca-4d93-969c-d72f7a2a1990",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Load .h5ad file\n",
        "# adata = sc.read_h5ad('/content/drive/MyDrive/Documents/COVID19_ALL.h5ad', backed='r') # Load only metadata (not full matrix)\n",
        "adata = sc.read_h5ad('/content/drive/MyDrive/Documents/SjD.h5ad')\n",
        "\n",
        "# print(adata.obs.columns)\n",
        "# print(adata.obs.head())\n",
        "# adata = sc.read_10x_mtx('C:/Users/User/decidua/sick', var_names='gene_symbols', cache=True) # Load from folder with 3 10x files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SCogKohqjWQc",
      "metadata": {
        "id": "SCogKohqjWQc"
      },
      "outputs": [],
      "source": [
        "# Select cells where 'celltype' is 'T_CD4_c12-FOXP3'\n",
        "# adata_backed = sc.read_h5ad('/content/drive/MyDrive/Documents/COVID19_ALL.h5ad', backed='r') # Load only metadata (not full matrix)\n",
        "# treg_ids = adata_backed.obs[adata_backed.obs[\"celltype\"] == \"T_CD4_c12-FOXP3\"].index\n",
        "adata = adata_backed[treg_ids, :].to_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XeaW6iRbFWP4",
      "metadata": {
        "id": "XeaW6iRbFWP4"
      },
      "source": [
        "### Load from URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcV9h9hLFgWY",
      "metadata": {
        "id": "IcV9h9hLFgWY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Download the file\n",
        "url = 'https://datasets.cellxgene.cziscience.com/0ce62d4c-3f6d-4600-a5a4-8aeb24c7fe09.h5ad'\n",
        "filename = 'FL.h5ad'\n",
        "\n",
        "if not os.path.exists(filename):  # Avoid re-downloading\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "                f.write(chunk)\n",
        "\n",
        "# Step 2: Open in backed mode\n",
        "adata = sc.read_h5ad (filename)\n",
        "\n",
        "# # Step 3: Create mask for lineage == \"TNK\"\n",
        "# mask = adata_backed.obs['lineage'] == 'TNK'\n",
        "\n",
        "# # Step 4: Subset and load into memory\n",
        "# adata_tnk = adata_backed[mask].to_memory()\n",
        "\n",
        "# # Step 5: Done!\n",
        "# print(adata_tnk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f2547e0-91bd-4edf-b982-be1bb95ecae9",
      "metadata": {
        "id": "3f2547e0-91bd-4edf-b982-be1bb95ecae9"
      },
      "source": [
        "### load 10x triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8vLNo3nII_Ir",
      "metadata": {
        "id": "8vLNo3nII_Ir"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/Documents/uveal_melanoma'   # â† your folder\n",
        "prefixes = sorted({ '_'.join(os.path.basename(f).split('_')[:2])\n",
        "                    for f in glob.glob(os.path.join(data_dir, '*_matrix.mtx.gz')) })\n",
        "\n",
        "adata_all, adata, adatas, timings = [], [], [], []\n",
        "\n",
        "for p in prefixes:\n",
        "    t0 = time.time()\n",
        "    matrix_f   = os.path.join(data_dir, f'{p}_matrix.mtx.gz')\n",
        "    barcodes_f = os.path.join(data_dir, f'{p}_barcodes.tsv.gz')\n",
        "    genes_f    = os.path.join(data_dir, f'{p}_genes.tsv.gz')\n",
        "    print(f'â³ Reading {p} â€¦')\n",
        "\n",
        "    # --- (A) simplest: use Scanpy's 10x reader onâ€‘theâ€‘fly -------------------\n",
        "    adata = sc.read_mtx(matrix_f).T                # cells as rows\n",
        "    adata.var_names = pd.read_csv(genes_f, header=None, sep='\\t')[0].values\n",
        "    adata.obs_names = pd.read_csv(barcodes_f, header=None)[0].values\n",
        "\n",
        "    # metadata\n",
        "    gsm_id, sample_id = p.split('_', 1)\n",
        "    adata.obs['gsm_id']    = gsm_id\n",
        "    adata.obs['sample_id'] = sample_id\n",
        "    adata.obs['source']    = p\n",
        "\n",
        "    adatas.append(adata)\n",
        "    print(f'   âœ… {adata.n_obs} cells Ã— {adata.n_vars} genes '\n",
        "          f'({time.time()-t0:.1f}s)')\n",
        "\n",
        "# ---------- concatenate every sample ----------------------------------------\n",
        "print('\\nðŸ”— Concatenating â€¦')\n",
        "adata_all = ad.concat(adatas,\n",
        "                      label='sample',\n",
        "                      keys=[a.obs[\"source\"][0] for a in adatas],\n",
        "                      index_unique='-')\n",
        "print(f'ðŸŽ‰ Done: {adata_all.shape} total')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UUcWs1WVNiTc",
      "metadata": {
        "id": "UUcWs1WVNiTc"
      },
      "source": [
        "### Load 10x format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Q5W-q6kNgBg",
      "metadata": {
        "id": "6Q5W-q6kNgBg"
      },
      "outputs": [],
      "source": [
        "# Set base directory\n",
        "data_dir = '/content/drive/MyDrive/Documents/adataelanoma'\n",
        "\n",
        "# Locate matrix files\n",
        "matrix_files = glob.glob(os.path.join(data_dir, '*_matrix.mtx.gz'))\n",
        "\n",
        "# Extract sample info\n",
        "sample_metadata = []\n",
        "pattern = re.compile(r'^(GSM\\d+)_MEL(\\d+)_(TIL|PBMC)_matrix\\.mtx\\.gz$')\n",
        "\n",
        "for path in matrix_files:\n",
        "    filename = os.path.basename(path)\n",
        "    match = pattern.match(filename)\n",
        "    if match:\n",
        "        gsm, mel_num, tissue = match.groups()\n",
        "        prefix = filename.replace('_matrix.mtx.gz', '_')  # âœ… trailing underscore included\n",
        "        sample_metadata.append({\n",
        "            \"sample\": f\"MEL{mel_num}_{tissue}\",\n",
        "            \"gsm\": gsm,\n",
        "            \"mel_id\": f\"MEL{mel_num}\",\n",
        "            \"tissue\": tissue,\n",
        "            \"prefix\": prefix\n",
        "        })\n",
        "\n",
        "# Load datasets\n",
        "adatas = []\n",
        "timings = []\n",
        "\n",
        "for meta in sample_metadata:\n",
        "    prefix = meta[\"prefix\"]\n",
        "    print(f\"â³ Reading {prefix}...\")\n",
        "    start = time.time()\n",
        "\n",
        "    try:\n",
        "        adata = sc.read_10x_mtx(\n",
        "            data_dir,\n",
        "            var_names='gene_symbols',\n",
        "            cache=False,\n",
        "            prefix=prefix\n",
        "        )\n",
        "\n",
        "        # Add metadata\n",
        "        for k, v in meta.items():\n",
        "            adata.obs[k] = v\n",
        "\n",
        "        adatas.append(adata)\n",
        "        duration = time.time() - start\n",
        "        print(f\"âœ… Loaded {adata.n_obs} cells, {adata.n_vars} genes in {duration:.2f} sec\")\n",
        "        timings.append((meta[\"sample\"], adata.n_obs, adata.n_vars, duration))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed on {prefix}: {e}\")\n",
        "\n",
        "# Concatenate\n",
        "if adatas:\n",
        "    print(\"\\nðŸ”— Concatenating all AnnData objects...\")\n",
        "    adata_all = sc.concat(adatas, label='sample', keys=[a.obs['sample'][0] for a in adatas])\n",
        "    print(f\"ðŸŽ‰ Final shape: {adata_all.shape}\")\n",
        "else:\n",
        "    print(\"âŒ No datasets loaded.\")\n",
        "\n",
        "# Summary tables\n",
        "df_time = pd.DataFrame(timings, columns=[\"sample\", \"n_cells\", \"n_genes\", \"time_sec\"])\n",
        "df_meta = pd.DataFrame(sample_metadata)\n",
        "\n",
        "print(\"\\nðŸ“Š Load time summary:\")\n",
        "display(df_time)\n",
        "print(\"\\nðŸ§¬ Sample metadata:\")\n",
        "display(df_meta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bXT2LiSjtSKs",
      "metadata": {
        "id": "bXT2LiSjtSKs"
      },
      "source": [
        "/content/drive/MyDrive/Documents/adataelanoma/GSM7764412_29_MEL12_TIL_barcodes.tsv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IpwSa3DPOpmw",
      "metadata": {
        "id": "IpwSa3DPOpmw"
      },
      "outputs": [],
      "source": [
        "adata=adata_all"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fAvWVeowLly-",
      "metadata": {
        "id": "fAvWVeowLly-"
      },
      "source": [
        "### \"Line too long\" matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Lx4UiZ-JKpb",
      "metadata": {
        "id": "6Lx4UiZ-JKpb"
      },
      "outputs": [],
      "source": [
        "import os, glob, time\n",
        "import pandas as pd\n",
        "from anndata import AnnData\n",
        "from scipy.sparse import coo_matrix\n",
        "import anndata as ad\n",
        "\n",
        "def manual_mtx_loader_without_shape(mtx_path):\n",
        "    \"\"\"Load a .mtx file without a shape line.\"\"\"\n",
        "    data, rows, cols = [], [], []\n",
        "    with open(mtx_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('%') or not line.strip():\n",
        "                continue\n",
        "            r, c, v = map(int, line.strip().split())\n",
        "            rows.append(r - 1)\n",
        "            cols.append(c - 1)\n",
        "            data.append(v)\n",
        "    n_rows = max(rows) + 1\n",
        "    n_cols = max(cols) + 1\n",
        "    return coo_matrix((data, (rows, cols)), shape=(n_rows, n_cols)).tocsr()\n",
        "\n",
        "# --- Main loading loop ---\n",
        "\n",
        "data_dir = '/content/AGVHD_unzipped'\n",
        "matrix_files = sorted(glob.glob(os.path.join(data_dir, '*_matrix.mtx')))\n",
        "\n",
        "adatas = []\n",
        "sample_ids = []\n",
        "timings = []\n",
        "\n",
        "for matrix_file in matrix_files:\n",
        "    start = time.time()\n",
        "    try:\n",
        "        basename = os.path.basename(matrix_file)\n",
        "        sample_id = basename.replace('_GEX_matrix.mtx', '')\n",
        "        print(f\"â³ Reading {sample_id}...\")\n",
        "\n",
        "        # Associated file paths\n",
        "        barcodes_file = os.path.join(data_dir, f'{sample_id}_GEX_barcodes.tsv')\n",
        "        genes_file = os.path.join(data_dir, f'{sample_id}_GEX_genes.tsv')\n",
        "\n",
        "        # Load matrix and transpose to (cells Ã— genes)\n",
        "        matrix = manual_mtx_loader_without_shape(matrix_file).T\n",
        "\n",
        "        # Load barcodes\n",
        "        with open(barcodes_file, 'r') as f:\n",
        "            barcodes = [line.strip() for line in f][:matrix.shape[0]]\n",
        "\n",
        "        # Load genes\n",
        "        with open(genes_file, 'r') as f:\n",
        "            lines = [line.strip().split('\\t') for line in f]\n",
        "            gene_ids = [x[0] for x in lines]\n",
        "            gene_names = [x[1] if len(x) > 1 else x[0] for x in lines]\n",
        "        gene_ids = gene_ids[:matrix.shape[1]]\n",
        "        gene_names = gene_names[:matrix.shape[1]]\n",
        "\n",
        "        # Deduplicate gene names\n",
        "        seen = set()\n",
        "        gene_names_unique = []\n",
        "        for name in gene_names:\n",
        "            if name in seen:\n",
        "                i = 1\n",
        "                new_name = f\"{name}_{i}\"\n",
        "                while new_name in seen:\n",
        "                    i += 1\n",
        "                    new_name = f\"{name}_{i}\"\n",
        "                name = new_name\n",
        "            seen.add(name)\n",
        "            gene_names_unique.append(name)\n",
        "\n",
        "        # Create AnnData\n",
        "        adata = AnnData(matrix)\n",
        "        adata.obs_names = barcodes\n",
        "        adata.var_names = gene_names_unique\n",
        "        adata.var['gene_ids'] = gene_ids\n",
        "        adata.obs['sample'] = sample_id\n",
        "        adata.obs['source_file'] = basename\n",
        "\n",
        "        adatas.append(adata)\n",
        "        sample_ids.append(sample_id)\n",
        "\n",
        "        duration = time.time() - start\n",
        "        print(f\"âœ… {sample_id} loaded in {duration:.2f} sec â€” {adata.shape[0]} cells, {adata.shape[1]} genes\")\n",
        "        timings.append((sample_id, adata.shape[0], adata.shape[1], duration))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed on {sample_id}: {e}\")\n",
        "\n",
        "# --- Merge all samples ---\n",
        "\n",
        "if adatas:\n",
        "    print(\"ðŸ”— Concatenating all AnnData objects...\")\n",
        "    adata_all = ad.concat(adatas, join='outer', label='file', keys=sample_ids)\n",
        "    print(f\"ðŸŽ‰ All data loaded: {adata_all.shape}\")\n",
        "else:\n",
        "    print(\"âŒ No files loaded successfully.\")\n",
        "\n",
        "# --- Load time summary ---\n",
        "df_time = pd.DataFrame(timings, columns=[\"sample\", \"n_cells\", \"n_genes\", \"time_sec\"])\n",
        "df_time[\"MB\"] = [os.path.getsize(f) / 1e6 for f in matrix_files]\n",
        "df_time[\"MB/sec\"] = df_time[\"MB\"] / df_time[\"time_sec\"]\n",
        "print(\"\\nðŸ“Š Load time summary:\")\n",
        "display(df_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P_Jx3Y9HQdZq",
      "metadata": {
        "id": "P_Jx3Y9HQdZq"
      },
      "outputs": [],
      "source": [
        "adata_all.write_h5ad('AGVHD.h5ad')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eba92a2-32d9-4b4b-925f-85be45af65f4",
      "metadata": {
        "id": "8eba92a2-32d9-4b4b-925f-85be45af65f4"
      },
      "source": [
        "### load 10x doublets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695ebe9f-9ec7-49e8-a7bd-396877b4cca4",
      "metadata": {
        "id": "695ebe9f-9ec7-49e8-a7bd-396877b4cca4"
      },
      "outputs": [],
      "source": [
        "adata = load_10x_doublets  (folder_path = '/content/drive/MyDrive/Documents/zhang')\n",
        "adata.raw = adata.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4bdb61-b32a-404e-b4e6-97b8cf069304",
      "metadata": {
        "id": "3b4bdb61-b32a-404e-b4e6-97b8cf069304"
      },
      "outputs": [],
      "source": [
        "#merge datasets\n",
        "# Concatenate datasets, add batch labels\n",
        "# adata = adata1.concatenate(adata2, batch_key='batch', batch_categories=['sample1', 'sample2']) # merge with batch info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c88eb3-0d63-4177-a548-ec66b42ef98a",
      "metadata": {
        "id": "33c88eb3-0d63-4177-a548-ec66b42ef98a"
      },
      "outputs": [],
      "source": [
        "\n",
        "  # load 10x triplets\n",
        "\n",
        "  # Load annotation file\n",
        "  annotations = pd.read_csv('GSE214695_cell_annotation.csv', index_col=0)\n",
        "\n",
        "  # Extract barcode (last part after '_') from cell_id\n",
        "  annotations['barcode'] = annotations['cell_id'].str.extract(r'([ACGT]+-\\d+)$')\n",
        "\n",
        "  # Drop duplicates â€” keep the first occurrence\n",
        "  annotations = annotations.drop_duplicates(subset='barcode')\n",
        "  annotations = annotations.set_index('barcode')\n",
        "\n",
        "  # Extract barcode from adata.obs_names\n",
        "  adata.obs['barcode'] = adata.obs_names.str.split('_').str[-1]\n",
        "\n",
        "  # Initialize with 'unknown'\n",
        "  adata.obs['annotation'] = 'unknown'\n",
        "  adata.obs['nanostring_reference'] = 'unknown'\n",
        "\n",
        "  # Map values from annotations DataFrame using barcode\n",
        "  adata.obs['annotation'] = adata.obs['barcode'].map(annotations['annotation']).fillna('unknown')\n",
        "  adata.obs['nanostring_reference'] = adata.obs['barcode'].map(annotations['nanostring_reference']).fillna('unknown')\n",
        "\n",
        "  # Optional: remove barcode column now that mapping is done\n",
        "  adata.obs.drop(columns=['barcode'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5LFcdhuwkrBG",
      "metadata": {
        "id": "5LFcdhuwkrBG"
      },
      "source": [
        "### Load from text matrix files (in batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eMSVb-5gPOF1",
      "metadata": {
        "id": "eMSVb-5gPOF1"
      },
      "outputs": [],
      "source": [
        "# Set your directory path\n",
        "data_dir = '/content/drive/MyDrive/Documents/HBV'  # replace with your actual path\n",
        "save_dir = '/content/processed_batches/'  # ðŸ” Where to save each batch\n",
        "final_out = 'HBV.h5ad'              # ðŸ” Final output filename\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(data_dir, '*.txt.gz')))\n",
        "batch_size = 8\n",
        "timings = []\n",
        "\n",
        "# === Function to read one expression file ===\n",
        "def read_matrix_sparse(file_path):\n",
        "    with gzip.open(file_path, 'rt') as f:\n",
        "        barcodes = f.readline().strip().replace('\"', '').split()\n",
        "        df = pd.read_csv(f, sep=' ', header=None, engine='c').dropna(how='all', axis=1)\n",
        "    genes = df.iloc[:, 0].values\n",
        "    matrix = csr_matrix(df.iloc[:, 1:].astype(np.float32).values.T)\n",
        "    return matrix, barcodes, genes\n",
        "\n",
        "# === Process in batches ===\n",
        "for batch_start in range(0, len(files), batch_size):\n",
        "    batch_files = files[batch_start:batch_start + batch_size]\n",
        "    batch_adatas = []\n",
        "    print(f\"\\nðŸ“¦ Processing batch {batch_start // batch_size} â€” files {batch_start} to {batch_start + len(batch_files) - 1}\")\n",
        "\n",
        "    for file in batch_files:\n",
        "        start = time.time()\n",
        "        basename = os.path.basename(file)\n",
        "        parts = basename.replace('.txt.gz', '').split('_')\n",
        "        gsm_id, sample_id, replicate = parts[0], parts[1], '_'.join(parts[2:])\n",
        "\n",
        "        print(f\"â³ Reading {basename}...\")\n",
        "        try:\n",
        "            matrix, barcodes, genes = read_matrix_sparse(file)\n",
        "            adata_i = AnnData(matrix)\n",
        "            adata_i.obs_names = barcodes\n",
        "            adata_i.var_names = genes\n",
        "            adata_i.obs['gsm_id'] = gsm_id\n",
        "            adata_i.obs['sample_id'] = sample_id\n",
        "            adata_i.obs['replicate'] = replicate\n",
        "            adata_i.obs['source_file'] = basename\n",
        "            batch_adatas.append(adata_i)\n",
        "\n",
        "            duration = time.time() - start\n",
        "            print(f\"âœ… {basename} loaded in {duration:.2f}s â€” {matrix.shape[0]} cells, {matrix.shape[1]} genes\")\n",
        "            timings.append((basename, matrix.shape[0], matrix.shape[1], duration))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed on {basename}: {e}\")\n",
        "\n",
        "    if batch_adatas:\n",
        "        print(f\"ðŸ’¾ Saving batch {batch_start // batch_size}...\")\n",
        "        adata_batch = ad.concat(\n",
        "            batch_adatas,\n",
        "            label='source_file',\n",
        "            keys=[a.obs['source_file'][0] for a in batch_adatas],\n",
        "            index_unique='-',\n",
        "            merge='same'\n",
        "        )\n",
        "        out_path = os.path.join(save_dir, f\"batch_{batch_start // batch_size}.h5ad\")\n",
        "        adata_batch.write(out_path, compression=\"gzip\")\n",
        "        print(f\"âœ… Saved {out_path} with shape {adata_batch.shape}\")\n",
        "\n",
        "    del batch_adatas, adata_batch, matrix\n",
        "    gc.collect()\n",
        "\n",
        "# === Merge all saved batches into final object ===\n",
        "print(\"\\nðŸ”— Merging all saved batches...\")\n",
        "batch_files = sorted(glob.glob(os.path.join(save_dir, 'batch_*.h5ad')))\n",
        "adata = ad.concat(\n",
        "    [ad.read_h5ad(f) for f in batch_files],\n",
        "    label=\"batch\",\n",
        "    keys=[os.path.basename(f).replace(\".h5ad\", \"\") for f in batch_files],\n",
        "    index_unique='-'\n",
        ")\n",
        "\n",
        "adata.write(final_out, compression=\"gzip\")\n",
        "print(f\"âœ… Final merged file saved as {final_out} â€” shape: {adata.shape}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Paths ===\n",
        "data_dir = '/content/drive/MyDrive/Documents/adataelanoma_2'\n",
        "final_out = 'adataelanoma_merged.h5ad'\n",
        "\n",
        "# === Load and annotate each file ===\n",
        "adatas = []\n",
        "for file in sorted(glob.glob(os.path.join(data_dir, '*.h5'))):\n",
        "    basename = os.path.basename(file)\n",
        "    gsm_id, sample_id = basename.replace('_filtered_feature_bc_matrix.h5', '').split('_')\n",
        "\n",
        "    print(f\"â³ Reading {basename}...\")\n",
        "    adata = sc.read_10x_h5(file)\n",
        "    adata.var_names_make_unique()\n",
        "\n",
        "    adata.obs['gsm_id'] = gsm_id\n",
        "    adata.obs['sample_id'] = sample_id\n",
        "    adata.obs['source_file'] = basename\n",
        "    adatas.append(adata)\n",
        "\n",
        "# === Merge all into one AnnData object ===\n",
        "print(\"ðŸ”— Concatenating all datasets...\")\n",
        "adataerged = ad.concat(\n",
        "    adatas,\n",
        "    label='source_file',\n",
        "    keys=[a.obs['source_file'][0] for a in adatas],\n",
        "    index_unique='-',\n",
        "    merge='same'\n",
        ")\n",
        "\n",
        "# === Save final output ===\n",
        "adataerged.write(final_out, compression='gzip')\n",
        "print(f\"âœ… Merged dataset saved to {final_out} â€” shape: {adataerged.shape}\")\n"
      ],
      "metadata": {
        "id": "PfLGycOUBsLo"
      },
      "id": "PfLGycOUBsLo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "57a68bd9-cb3d-477d-b94a-2e0f804ac91b",
      "metadata": {
        "id": "57a68bd9-cb3d-477d-b94a-2e0f804ac91b"
      },
      "source": [
        "## QC"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xmH7QNQ4lv5t",
      "metadata": {
        "id": "xmH7QNQ4lv5t"
      },
      "source": [
        "### Compute QC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See if any genes start with \"MT-\"\n",
        "mt_genes = adata.var_names.str.startswith('MT-')\n",
        "print(f\"Mitochondrial genes found: {mt_genes.sum()}\")\n"
      ],
      "metadata": {
        "id": "yc9QwewxfKiI"
      },
      "id": "yc9QwewxfKiI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uqj_hahWlvfR",
      "metadata": {
        "id": "uqj_hahWlvfR"
      },
      "outputs": [],
      "source": [
        "adata = compute_qc (adata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T7dJE0BGl3Kr",
      "metadata": {
        "id": "T7dJE0BGl3Kr"
      },
      "source": [
        "### Display QC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf4a260c-1dff-441a-b56b-c22cc1111364",
      "metadata": {
        "id": "cf4a260c-1dff-441a-b56b-c22cc1111364",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "  plt.rcParams['figure.figsize'] = (5, 5)\n",
        "  sc.pl.umap(adata, color=['leiden_1','FastTag'],legend_loc='on data', size=25,legend_fontsize=12)\n",
        "  plt.rcParams['figure.figsize'] = (6, 5)\n",
        "  sc.pl.violin(\n",
        "      adata=adata,\n",
        "      keys=['pct_counts_mt', 'pct_counts_ribo', 'pct_counts_hsp', 'n_genes_by_counts', 'total_counts'],\n",
        "      groupby='leiden_1', stripplot=False, jitter=False, rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rtbtEHg8mDEZ",
      "metadata": {
        "id": "rtbtEHg8mDEZ"
      },
      "source": [
        "### Apply QC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LjQH94XrUDp7",
      "metadata": {
        "id": "LjQH94XrUDp7"
      },
      "outputs": [],
      "source": [
        "## filter non-cells\n",
        "print (\"Before: \"); print (adata.shape)\n",
        "sc.pp.filter_cells(adata, min_genes=300)\n",
        "print (\"After: \"); print (adata.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75139cd4-ec63-48da-bea7-c62bf4dea33e",
      "metadata": {
        "id": "75139cd4-ec63-48da-bea7-c62bf4dea33e"
      },
      "outputs": [],
      "source": [
        "# QC subset-------------------------------------------------\n",
        "\n",
        "# Define QC filters\n",
        "qc_pass = (\n",
        "    (adata.obs['pct_counts_mt'] <= 2.5) &\n",
        "    (adata.obs['pct_counts_ribo'] >= 5) &\n",
        "    (adata.obs['pct_counts_hsp'] <= 1.5) &\n",
        "    (adata.obs['n_genes_by_counts'] >= 800) &\n",
        "    (adata.obs['total_counts'] >= 1500)\n",
        "    )\n",
        "\n",
        "# Before / After counts\n",
        "total_before = adata.n_obs\n",
        "total_after = qc_pass.sum()\n",
        "\n",
        "# Counts per FastTag category\n",
        "before_counts = adata.obs['leiden_1'].value_counts().sort_index()\n",
        "after_counts = (\n",
        "    adata.obs.loc[qc_pass, 'leiden_1']\n",
        "    .value_counts()\n",
        "    .reindex(before_counts.index)\n",
        "    .fillna(0)\n",
        "    .astype(int))\n",
        "\n",
        "# Create and print report\n",
        "report = pd.DataFrame({\n",
        "    'before': before_counts,\n",
        "    'after': after_counts,\n",
        "    'retained_%': (after_counts / before_counts * 100).round(2).astype(str) + '%'\n",
        "})\n",
        "\n",
        "print(f'Total cells before QC: {total_before:,}')\n",
        "print(f'Total cells after QC: {total_after:,} ({total_after / total_before:.1%} retained)\\n')\n",
        "print(report)\n",
        "\n",
        "# Confirm before subsetting\n",
        "confirm = input('\\nProceed with QC subset? (y/n): ')\n",
        "\n",
        "if confirm.lower() == 'y':\n",
        "    adata = adata[qc_pass].copy()\n",
        "    print('\\nQC filtering applied and data subsetted.')\n",
        "else:\n",
        "    print('\\nQC filtering not applied. Data remains unchanged.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daaa45ec-762b-4352-a6ac-6fe769a0693c",
      "metadata": {
        "id": "daaa45ec-762b-4352-a6ac-6fe769a0693c"
      },
      "source": [
        "## Tweaks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8lljEF085Ihe",
      "metadata": {
        "id": "8lljEF085Ihe"
      },
      "source": [
        "### adata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dJb5ut195CkR",
      "metadata": {
        "id": "dJb5ut195CkR"
      },
      "outputs": [],
      "source": [
        "adata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adata = convert ( adata)"
      ],
      "metadata": {
        "id": "3dVRPJbJBNZq"
      },
      "id": "3dVRPJbJBNZq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "009e9108-35b3-432c-bb63-00776f31804a",
      "metadata": {
        "id": "009e9108-35b3-432c-bb63-00776f31804a"
      },
      "source": [
        "### Recompute clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7b9e83-568b-44bc-890f-3854062afa8a",
      "metadata": {
        "id": "5f7b9e83-568b-44bc-890f-3854062afa8a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "  # sc.pp.neighbors(adata, use_rep='X_pca_harmony'),\n",
        "  # sc.tl.umap(adata)\n",
        "  sc.tl.leiden(adata, resolution=2, key_added='leiden_2',flavor = 'igraph',n_iterations=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa63ac24-c3eb-4f9d-a158-c119f24fb5e2",
      "metadata": {
        "id": "aa63ac24-c3eb-4f9d-a158-c119f24fb5e2"
      },
      "source": [
        "### Remove non-coding genes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c69c1385-5ca2-4f6c-9837-126bebfd4e3f",
      "metadata": {
        "id": "c69c1385-5ca2-4f6c-9837-126bebfd4e3f"
      },
      "outputs": [],
      "source": [
        "# Remove non-coding genes\n",
        "gc.collect()\n",
        "# print (adata.shape)\n",
        "# adata = coding_genes (adata)\n",
        "# print (adata.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e39e74-b06e-41d2-a7f8-f4d4bf0cd0bb",
      "metadata": {
        "id": "36e39e74-b06e-41d2-a7f8-f4d4bf0cd0bb"
      },
      "source": [
        "### Rename categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dcf1f2a-37a5-4324-86ac-c4738ab3d11e",
      "metadata": {
        "id": "5dcf1f2a-37a5-4324-86ac-c4738ab3d11e"
      },
      "outputs": [],
      "source": [
        "  # Rename categories\n",
        "  # adata.obs['manual_annotation'] = adata.obs['manual_annotation'].cat.add_categories(['circ_CD56_NK'])\n",
        "  # adata.obs.loc[adata.obs['FastTag'] == 'naive_Treg', 'FastTag'] = 'Treg'\n",
        "  adata.obs['treatment'] = adata.obs['timepoint'].apply(lambda x: 'treated' if x in ['d7', 'd42'] else 'untreated')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename gene\n",
        "adata.var_names = adata.var_names.to_series().replace({\"FOXP3_1\": \"FOXP3\"})"
      ],
      "metadata": {
        "id": "9IJd6LklA6sM"
      },
      "id": "9IJd6LklA6sM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ba4f6692-05b9-4fc5-8d17-93f61d34a3aa",
      "metadata": {
        "id": "ba4f6692-05b9-4fc5-8d17-93f61d34a3aa"
      },
      "source": [
        "### Delete cell categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5965aedd-da8b-4f87-a133-3cca6cc25c5a",
      "metadata": {
        "id": "5965aedd-da8b-4f87-a133-3cca6cc25c5a"
      },
      "outputs": [],
      "source": [
        "  # adata = adata[~adata.obs['author_cell_type'].str.endswith('-Doublet')].copy()\n",
        "  # adata = adata[adata.obs['leiden_1'].isin(['5'])].copy() # retain\n",
        "  adata = adata[~adata.obs['leiden_1'].isin(['11'])].copy() # remove\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d430265c-5d12-420b-8c65-0b3530ccf0fa",
      "metadata": {
        "id": "d430265c-5d12-420b-8c65-0b3530ccf0fa"
      },
      "source": [
        "### split into dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d64e3ed-4bb9-447b-beda-7c58367097b7",
      "metadata": {
        "id": "8d64e3ed-4bb9-447b-beda-7c58367097b7"
      },
      "outputs": [],
      "source": [
        "adata_full=adata\n",
        "adata_dict = split(adata, category = 'FastTag', min_cells=30)\n",
        "adata = adata_dict ['T_NK']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QYQsqe88kmz7",
      "metadata": {
        "id": "QYQsqe88kmz7"
      },
      "source": [
        "### Check shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wpwpowS2kvd9",
      "metadata": {
        "id": "wpwpowS2kvd9"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape of adata.X: {adata.X.shape}\")\n",
        "if adata.raw is None:\n",
        "    print(\"adata.raw is None. Raw data is not available.\")\n",
        "else:\n",
        "    print(f\"Shape of adata.raw.X: {adata.raw.X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AA7E78RxlcE9",
      "metadata": {
        "id": "AA7E78RxlcE9"
      },
      "source": [
        "### Pull X from raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JHfRNRzylhRm",
      "metadata": {
        "id": "JHfRNRzylhRm"
      },
      "outputs": [],
      "source": [
        "adata = restore_from_raw(adata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U0JQL4PwjBlA",
      "metadata": {
        "id": "U0JQL4PwjBlA"
      },
      "source": [
        "### Pull raw from X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-7ipshYejAfr",
      "metadata": {
        "id": "-7ipshYejAfr"
      },
      "outputs": [],
      "source": [
        "def has_nans(X):\n",
        "    if sp.issparse(X):\n",
        "        # Convert only the data (non-zero entries) to dense array for NaN check\n",
        "        return np.isnan(X.data).any()\n",
        "    else:\n",
        "        return np.isnan(X).any()\n",
        "\n",
        "if has_nans(adata.X):\n",
        "    print(\"âš ï¸ Warning: adata.X contains NaNs. Don't set this as raw.\")\n",
        "else:\n",
        "    adata.raw = adata.copy()\n",
        "    print(\"âœ… adata.raw set successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kL7pBiqe_BoQ",
      "metadata": {
        "id": "kL7pBiqe_BoQ"
      },
      "source": [
        "### Remove unused categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fqXTSAXIA2ZD",
      "metadata": {
        "id": "fqXTSAXIA2ZD"
      },
      "outputs": [],
      "source": [
        "adata.obs['FastTag'] = adata.obs['FastTag'].cat.remove_unused_categories()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge datasets"
      ],
      "metadata": {
        "id": "ph5neU4xYNRL"
      },
      "id": "ph5neU4xYNRL"
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_ds(adata, adata):\n",
        "    import pandas as pd\n",
        "\n",
        "    adata.obs_names_make_unique()\n",
        "    adata.obs_names_make_unique()\n",
        "\n",
        "    adata.obs['dataset_origin'] = 'A'\n",
        "    adata.obs['dataset_origin'] = 'H'\n",
        "\n",
        "    adata.obs = adata.obs.rename(columns={\n",
        "        'Sample_ID': 'sample',\n",
        "        'COND': 'condition'\n",
        "    })\n",
        "\n",
        "    # Ensure existence and fill where missing (with categorical safety)\n",
        "    if 'condition' not in adata.obs:\n",
        "        adata.obs['condition'] = 'T1D'\n",
        "    elif pd.api.types.is_categorical_dtype(adata.obs['condition']):\n",
        "        adata.obs['condition'] = adata.obs['condition'].cat.add_categories(['T1D'])\n",
        "        adata.obs['condition'] = adata.obs['condition'].fillna('T1D')\n",
        "    else:\n",
        "        adata.obs['condition'] = adata.obs['condition'].fillna('T1D')\n",
        "\n",
        "    if 'treatment' not in adata.obs:\n",
        "        adata.obs['treatment'] = 'untreated'\n",
        "    elif pd.api.types.is_categorical_dtype(adata.obs['treatment']):\n",
        "        adata.obs['treatment'] = adata.obs['treatment'].cat.add_categories(['untreated'])\n",
        "        adata.obs['treatment'] = adata.obs['treatment'].fillna('untreated')\n",
        "    else:\n",
        "        adata.obs['treatment'] = adata.obs['treatment'].fillna('untreated')\n",
        "\n",
        "    # Ensure both have all required obs columns\n",
        "    for col in ['sample', 'treatment', 'condition']:\n",
        "        if col not in adata.obs:\n",
        "            adata.obs[col] = pd.NA\n",
        "        if col not in adata.obs:\n",
        "            adata.obs[col] = pd.NA\n",
        "\n",
        "    # Outer join on genes\n",
        "    adata = ad.concat(\n",
        "        [adata, adata],\n",
        "        axis=0,\n",
        "        join='outer',\n",
        "        merge='same',\n",
        "        index_unique='-'\n",
        "    )\n",
        "\n",
        "    # Fill NaNs after concat\n",
        "    for col, fill_val in [('condition', 'T1D'), ('treatment', 'untreated')]:\n",
        "        if pd.api.types.is_categorical_dtype(adata.obs[col]):\n",
        "            if fill_val not in adata.obs[col].cat.categories:\n",
        "                adata.obs[col] = adata.obs[col].cat.add_categories([fill_val])\n",
        "        adata.obs[col] = adata.obs[col].fillna(fill_val)\n",
        "\n",
        "    # New group column\n",
        "    adata.obs['group'] = (\n",
        "        adata.obs['condition'].astype(str) + \"_\" + adata.obs['treatment'].astype(str)\n",
        "    )\n",
        "\n",
        "    return adata\n",
        "\n",
        "adata = merge_ds(adata, adata)"
      ],
      "metadata": {
        "id": "ZvUuNcddYJlU"
      },
      "id": "ZvUuNcddYJlU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a27ccc45-2d8c-4a29-8536-0a02a77eff13",
      "metadata": {
        "id": "a27ccc45-2d8c-4a29-8536-0a02a77eff13"
      },
      "source": [
        "## Annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5157f83-f17c-4f6f-ad71-5b16d404584b",
      "metadata": {
        "id": "d5157f83-f17c-4f6f-ad71-5b16d404584b"
      },
      "source": [
        "### DGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a62fc482-962d-4b24-8937-9c4e7c18e857",
      "metadata": {
        "id": "a62fc482-962d-4b24-8937-9c4e7c18e857"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = DGE(\n",
        "    adata= adata,\n",
        "    groupby='leiden_1',\n",
        "    target_group=['8'],\n",
        "    reference_group='rest',\n",
        "    min_log2fc=0.15,\n",
        "    min_delta_pct = 0.15,\n",
        "    min_pct = 0.15,\n",
        "    n_sample=5000,\n",
        "    n_genes=50,\n",
        "    return_df=True,\n",
        "    direction = \"pos\"\n",
        ")\n",
        "display (df.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "000cdaed-c21e-4f7c-ab95-b9688458e2fc",
      "metadata": {
        "id": "000cdaed-c21e-4f7c-ab95-b9688458e2fc"
      },
      "source": [
        "#### DotPlot and UMAPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "939e1b48-2066-4c4c-9780-d54497741af5",
      "metadata": {
        "id": "939e1b48-2066-4c4c-9780-d54497741af5"
      },
      "outputs": [],
      "source": [
        "\n",
        "genes = [\"leiden_1\"]  + list (df.head(20)['gene'])\n",
        "# sc.pl.dotplot(adata,var_names=genes,groupby='FastTag', standard_scale=None, figsize=(12, 5), cmap='Reds')\n",
        "sc.pl.umap(\n",
        "  adata=adata,\n",
        "  color= genes,\n",
        "  palette = set4,\n",
        "  use_raw = True,\n",
        "  legend_loc='on data', legend_fontsize=10, legend_fontweight='bold',\n",
        "  ncols=3, size=10, alpha=1,\n",
        "  vmax= None,\n",
        "  na_color='gray' # Explicitly set the missing value color\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kCamGNVseUQ6",
      "metadata": {
        "id": "kCamGNVseUQ6"
      },
      "outputs": [],
      "source": [
        "print (genes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0375eb8e-2c37-45fd-97ff-4a4337957603",
      "metadata": {
        "id": "0375eb8e-2c37-45fd-97ff-4a4337957603"
      },
      "source": [
        "\n",
        "#### Enrichment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91dee6af-db7c-4286-ba4b-f1c1fa360306",
      "metadata": {
        "id": "91dee6af-db7c-4286-ba4b-f1c1fa360306",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "genes = df['gene']\n",
        "results = enrich_all(genes)\n",
        "top50 = results.head(50)\n",
        "display(top50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72b53a59-0e97-4838-a08c-fd7c900d17d8",
      "metadata": {
        "id": "72b53a59-0e97-4838-a08c-fd7c900d17d8"
      },
      "source": [
        "#### ChatGPT query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b71b5fc-9f74-46b0-bdd7-c60eda0da128",
      "metadata": {
        "id": "2b71b5fc-9f74-46b0-bdd7-c60eda0da128"
      },
      "outputs": [],
      "source": [
        "\n",
        "display (ChatGPT (genes, 'TILs', 'follicular lymphoma'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V0b8YAk1vp_g",
      "metadata": {
        "id": "V0b8YAk1vp_g"
      },
      "outputs": [],
      "source": [
        "# prompt: form a new column called \"treatment\" which is equal \"IL2\" when timepoint is \"D7\" or 'D42' and equal 'none' when not\n",
        "\n",
        "adata.obs['treatment'] = adata.obs['timepoint'].apply(lambda x: 'treated' if x in ['d7', 'd42'] else 'untreated')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389f8699-f557-4be8-815f-9c5607d62b5a",
      "metadata": {
        "id": "389f8699-f557-4be8-815f-9c5607d62b5a"
      },
      "source": [
        "### Show cell counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "794e615d-c9e5-464c-baee-baa1c869ebac",
      "metadata": {
        "id": "794e615d-c9e5-464c-baee-baa1c869ebac"
      },
      "outputs": [],
      "source": [
        "# prompt: Display a table showing how many cells came from each patient at each timepoint\n",
        "\n",
        "table = cell_count_table(adata, row='FastTag', column=\"Sample\", display_full=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8115a177-2261-41b6-b58a-5c1024c2667d",
      "metadata": {
        "id": "8115a177-2261-41b6-b58a-5c1024c2667d"
      },
      "source": [
        "### confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87161281-6fe8-47be-8663-be2fedfff9bb",
      "metadata": {
        "id": "87161281-6fe8-47be-8663-be2fedfff9bb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "  # confusion matrix\n",
        "  # purge(adata, 'Sample', min_count=10) # Removed as 'annotation' column is missing\n",
        "  conf_mat = pd.crosstab(adata.obs['FastTag'], adata.obs['leiden_1']) # Changed 'annotation' to 'leiden_1'\n",
        "  # Step 2: Normalize rows to percentages\n",
        "  conf_mat_percent = conf_mat.div(conf_mat.sum(axis=1), axis=0) * 100\n",
        "  fig = plt.figure(figsize=(10, 12))\n",
        "  sns.set(font_scale=1.4)\n",
        "  sns.heatmap(conf_mat_percent, annot=True, fmt='.1f', cmap='Reds')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adata"
      ],
      "metadata": {
        "id": "5OIuEcghdaRK"
      },
      "id": "5OIuEcghdaRK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e2a8c711-d360-45d9-a06b-ee2cd2457239",
      "metadata": {
        "id": "e2a8c711-d360-45d9-a06b-ee2cd2457239"
      },
      "source": [
        "### Run pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea51e9a-083f-41f2-85cf-358790bc4aac",
      "metadata": {
        "id": "1ea51e9a-083f-41f2-85cf-358790bc4aac"
      },
      "outputs": [],
      "source": [
        "if (adata.raw.X.shape != adata.X.shape):\n",
        "  adata = restore_from_raw(adata)\n",
        "# sc.pp.normalize_total(adata=adata, target_sum=1e4)\n",
        "# sc.pp.log1p(adata)\n",
        "adata = convert (adata)\n",
        "# adata = coding_genes (adata)\n",
        "# adata.obs_names_make_unique()\n",
        "# adata = pipeline (adata, harmony=True, sample_key = 'sample_id',  n_top_genes=1000, downsample_n = 1000 )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70d2d15-407e-4a90-ac8f-6721619a1b20",
      "metadata": {
        "id": "a70d2d15-407e-4a90-ac8f-6721619a1b20"
      },
      "source": [
        "### Run FastTag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388b0f21-b8c3-4bee-a5f5-2a9fc4b5d797",
      "metadata": {
        "id": "388b0f21-b8c3-4bee-a5f5-2a9fc4b5d797"
      },
      "outputs": [],
      "source": [
        "# Run FastTag annotation\n",
        "\n",
        "adata = FastTag(adata=adata, ontology_yaml='coarse.yaml', # 'coarse.yaml','ontology_TIL.yaml'\n",
        "                markers_yaml='markers.yaml', smoothing_cycles=1,\n",
        "                majority_vote=True, root = 'root')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PO5kkEqjLTt3"
      },
      "id": "PO5kkEqjLTt3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f34ce4a1-1255-4208-8959-086a002577e2",
      "metadata": {
        "id": "f34ce4a1-1255-4208-8959-086a002577e2"
      },
      "source": [
        "## visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P9zKxi0EoVDC",
      "metadata": {
        "id": "P9zKxi0EoVDC"
      },
      "outputs": [],
      "source": [
        " if True:\n",
        "  purge (adata,\"FastTag\",30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dcbffe-ccdc-45a4-a6dd-1f1e86641b0c",
      "metadata": {
        "id": "e0dcbffe-ccdc-45a4-a6dd-1f1e86641b0c"
      },
      "source": [
        "### UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1303a446-44e1-49df-9043-cc4dca1eceec",
      "metadata": {
        "id": "1303a446-44e1-49df-9043-cc4dca1eceec"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = (10,10)\n",
        "# Define a color for missing values that is unlikely to be in the palette\n",
        "na_color = '#CCCCCC' # Use a light grey color\n",
        "\n",
        "sc.pl.umap(\n",
        "  adata=adata,\n",
        "  # color= ['FastTag'] + subscores ('T_NK',adata,ontology_yaml='ontology_TIL.yaml'),\n",
        "  # color = ['FastTag','disease'],\n",
        "  # color = markers('ex_CD8',adata=adata, sign = 'pos'),\n",
        "  # color= subscores ('cell_state',adata),\n",
        "  color= ['CD4', 'CD40LG', 'GZMA','GZMK', 'NKG7', 'EOMES'],\n",
        "  # palette = set4,\n",
        "  use_raw = True,\n",
        "  legend_loc='on data', legend_fontsize=20, legend_fontweight='regular',\n",
        "  ncols=3, size=15, alpha=1,\n",
        "  vmax= None,\n",
        "  na_color=na_color # Explicitly set the missing value color\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FastTag+UMAP"
      ],
      "metadata": {
        "id": "zwYmYfF3Ek_B"
      },
      "id": "zwYmYfF3Ek_B"
    },
    {
      "cell_type": "code",
      "source": [
        "adata = FastTag(adata=adata, ontology_yaml='ontology_TIL.yaml', # 'coarse.yaml','ontology_TIL.yaml'\n",
        "                markers_yaml='markers.yaml', smoothing_cycles=1,\n",
        "                majority_vote=True, root = 'T_NK')\n",
        "sc.pl.umap(\n",
        "  adata=adata,\n",
        "  color ='FastTag',\n",
        "  legend_loc='on data', legend_fontsize=10, legend_fontweight='regular',\n",
        "  use_raw = True, size = 20, ncols= 3)"
      ],
      "metadata": {
        "id": "95xgd9-rEh6o"
      },
      "id": "95xgd9-rEh6o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "HV3Rm-jc3NFB",
      "metadata": {
        "id": "HV3Rm-jc3NFB"
      },
      "source": [
        "### Plot ontology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TqOskMMU3KWe",
      "metadata": {
        "id": "TqOskMMU3KWe"
      },
      "outputs": [],
      "source": [
        "G, normalized_markers = init_ontology('ontology.yaml', 'markers.yaml', root='PBMC')\n",
        "palette = make_palette(lineage_to_cells, lineage_to_color)\n",
        "plot_ontology(G, root='PBMC', vertex_size=55, vertex_label_size=9, bbox=(1600,1200), palette=palette)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2_myiVuTtx81",
      "metadata": {
        "id": "2_myiVuTtx81"
      },
      "outputs": [],
      "source": [
        "adata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NpTR5jRJZYEX",
      "metadata": {
        "id": "NpTR5jRJZYEX"
      },
      "source": [
        "### Dot Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LVLxk7kNZP9I",
      "metadata": {
        "id": "LVLxk7kNZP9I"
      },
      "outputs": [],
      "source": [
        "# Dot Plot\n",
        "genes = ['CMTM6', 'BCL2', 'S100A4', 'IL2RA', 'IL32',\n",
        "'NME2', 'MYL12A', 'PIM1', 'CORO1B', 'FLT3LG',\n",
        "'CISH', 'PDE4B', 'TNFRSF1B', 'ENO1', 'CYLD',\n",
        "'LGALS1', 'ARL4C', 'ARPC1B', 'UPP1', 'NDUFV2',\n",
        "'ARL6IP5', 'GNG2', 'G3BP2', 'HIGD2A', 'CLIC1',\n",
        "'PCED1B', 'PIM2', 'RASSF5', 'COX8A', 'ARPC5L',\n",
        "'SOCS2', 'PPP1R18', 'PPM1G', 'LGALS3', 'METTL9',\n",
        "'RAB11FIP1', 'PTGER2', 'PAG1', 'CMTM3', 'PHTF2',\n",
        "'SGK1', 'UBALD2', 'NPDC1', 'ACTN4', 'DGKE',\n",
        "'SLC37A3', 'CBFB', 'STIP1', 'TNFRSF18', 'TNFRSF4']\n",
        "\n",
        "# Convert 'week' to string to avoid potential interval issues in dotplot\n",
        "# adata.obs['week_str'] = adata.obs['week'].astype(str)\n",
        "\n",
        "sc.pl.dotplot(adata, vmax= 1.5,\n",
        "              var_names=genes,\n",
        "              groupby='treatment', # Use the new string column for grouping\n",
        "              cmap='Greens',\n",
        "              standard_scale=None,  dendrogram=False, use_raw=True, figsize=(25, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gQaePEyzY8wR",
      "metadata": {
        "id": "gQaePEyzY8wR"
      },
      "source": [
        "### Gene heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076078a6-f84f-4cc5-99bc-6c77854a1854",
      "metadata": {
        "id": "076078a6-f84f-4cc5-99bc-6c77854a1854",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#gene heatmap\n",
        "\n",
        "sc.pl.matrixplot(adata, groupby = 'FastTag', dendrogram = True,\n",
        "                  var_names = ['TYROBP','XCL2', 'GZMB','CD3D','CD3G','CD8A','CD8B',\n",
        "                               'CCL5','GZMH','GZMK','NCR3','SLC4A10',\n",
        "                               'CD40LG', 'CD4','SELL','CCR7','TCF7','GATA3','ANXA1',\n",
        "                               'CXCR5', 'TOX2', 'CXCL13',\n",
        "                               'FOXP3', 'IL2RA', 'TIGIT','TYMS','MKI67']\n",
        "                  ,standard_scale = 'var')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228d35aa-f0e4-4652-b6b8-78445c519b74",
      "metadata": {
        "id": "228d35aa-f0e4-4652-b6b8-78445c519b74"
      },
      "source": [
        "## Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gaKeacNzxo2i",
      "metadata": {
        "id": "gaKeacNzxo2i"
      },
      "outputs": [],
      "source": [
        "auc_full = volcano_auc_from_raw(df=DGE_genes, fold_change_col='fold_change', pval_col='pval')\n",
        "print(\"Full-range volcano AUC:\", auc_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K4ZK4lyCkYj1",
      "metadata": {
        "id": "K4ZK4lyCkYj1"
      },
      "source": [
        "### T Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w5gOVl3XkXP1",
      "metadata": {
        "id": "w5gOVl3XkXP1"
      },
      "outputs": [],
      "source": [
        "pb_df = generate_pseudobulk_matrix(adata_dict, cell_type='Treg', groupby_cols=['patient', 'treatment_group'])\n",
        "\n",
        "# Run the fast paired t-test\n",
        "res = fast_filtered_paired_ttest(\n",
        "    pb_df,\n",
        "    cond1='PBMC',\n",
        "    cond2='TIL',\n",
        "    expr_threshold=0.1,       # at least 10% of treated patients must express the gene\n",
        "    fold_change_threshold=1.5 # fold change must be â‰¥ 1.5\n",
        ")\n",
        "\n",
        "# View the top differentially expressed genes\n",
        "res.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9bdf68",
      "metadata": {
        "id": "1f9bdf68"
      },
      "outputs": [],
      "source": [
        "# Purge FastTag categories with 0 cells\n",
        "print(\"FastTag categories before purging:\", adata.obs['FastTag'].cat.categories.tolist())\n",
        "initial_cell_count = adata.n_obs\n",
        "\n",
        "purge(adata, column='FastTag', min_count=1)\n",
        "\n",
        "print(\"FastTag categories after purging:\", adata.obs['FastTag'].cat.categories.tolist())\n",
        "print(f\"Number of cells before purging: {initial_cell_count}\")\n",
        "print(f\"Number of cells after purging: {adata.n_obs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IV2GsUtWpcDV",
      "metadata": {
        "id": "IV2GsUtWpcDV"
      },
      "outputs": [],
      "source": [
        "def paired_ttest_pseudobulk(pb_df, gene, group_col='treatment_group', subject_col='patient',\n",
        "                            cond1='untreated', cond2='treated', min_subjects=3):\n",
        "    \"\"\"\n",
        "    Run one-sided paired t-test (cond2 > cond1) on pseudobulk expression data.\n",
        "    \"\"\"\n",
        "    df = pb_df[[subject_col, group_col, gene]].dropna()\n",
        "\n",
        "    # Pivot to subject Ã— condition\n",
        "    df_wide = df.pivot(index=subject_col, columns=group_col, values=gene)\n",
        "\n",
        "    # Drop patients missing either condition\n",
        "    df_paired = df_wide.dropna()\n",
        "\n",
        "    if len(df_paired) < min_subjects:\n",
        "        print(f\"âŒ Not enough paired samples (found {len(df_paired)}). Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # Perform one-sided paired t-test: cond2 > cond1\n",
        "    t_stat, pval_two_sided = ttest_rel(df_paired[cond2], df_paired[cond1])\n",
        "    pval_one_sided = pval_two_sided / 2 if t_stat > 0 else 1.0\n",
        "\n",
        "    # Compute fold change\n",
        "    fc = (df_paired[cond2].mean() + 1e-6) / (df_paired[cond1].mean() + 1e-6)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'gene': [gene],\n",
        "        'fold_change': [fc],\n",
        "        'pval_one_sided': [pval_one_sided],\n",
        "        'n_pairs': [len(df_paired)]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u6cUR3vfpnQv",
      "metadata": {
        "id": "u6cUR3vfpnQv"
      },
      "outputs": [],
      "source": [
        "# Run test for one gene\n",
        "pb_df = generate_pseudobulk_matrix(adata, groupby_cols=['mel_id', 'tissue'])\n",
        "res = paired_ttest_pseudobulk(pb_df, gene='IL2_score')\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J2qNPYJJrNnT",
      "metadata": {
        "id": "J2qNPYJJrNnT"
      },
      "outputs": [],
      "source": [
        "pb_df = generate_pseudobulk_matrix(adata_dict, cell_type='Treg', groupby_cols=['patient', 'treatment_group'])\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "def group_ttest(pb_df, cond1='untreated', cond2='treated',\n",
        "                                    expr_threshold=0.1, fold_change_threshold=1.5,\n",
        "                                    subject_col='patient', group_col='treatment_group'):\n",
        "    \"\"\"\n",
        "    Run one-sided paired t-test for all genes, filtering by expression in cond2 and fold change.\n",
        "    Filters are ordered from least to most computationally expensive.\n",
        "    \"\"\"\n",
        "    gene_cols = pb_df.columns.difference([subject_col, group_col])\n",
        "    results = []\n",
        "\n",
        "    for gene in gene_cols:\n",
        "        # Prepare data for current gene\n",
        "        temp_df = pb_df[[subject_col, group_col, gene]].rename(columns={gene: 'expr'})\n",
        "        df_wide = temp_df.pivot(index=subject_col, columns=group_col, values='expr')\n",
        "\n",
        "        # Skip if conditions not both present\n",
        "        if cond1 not in df_wide.columns or cond2 not in df_wide.columns:\n",
        "            continue\n",
        "\n",
        "        # Drop patients missing either condition\n",
        "        df_wide = df_wide[[cond1, cond2]].dropna()\n",
        "\n",
        "        # Skip if not enough paired patients\n",
        "        if df_wide.shape[0] < 3:\n",
        "            continue\n",
        "\n",
        "        # Expression threshold in cond2\n",
        "        expr_pct = (df_wide[cond2] > 0).sum() / df_wide.shape[0]\n",
        "        if expr_pct < expr_threshold:\n",
        "            continue\n",
        "\n",
        "        # Run paired one-sided t-test (cond2 > cond1)\n",
        "        t_stat, pval = ttest_rel(df_wide[cond2], df_wide[cond1], alternative='greater')\n",
        "\n",
        "        # Compute fold change\n",
        "        fc = (df_wide[cond2].mean() + 1e-6) / (df_wide[cond1].mean() + 1e-6)\n",
        "        if fc < fold_change_threshold:\n",
        "            continue\n",
        "\n",
        "        # Store result\n",
        "        results.append({\n",
        "            'gene': gene,\n",
        "            'fold_change': fc,\n",
        "            'expr_pct_treated': expr_pct,\n",
        "            'pval': pval,\n",
        "            'n_pairs': df_wide.shape[0]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).sort_values('pval')\n",
        "\n",
        "\n",
        "\n",
        "res = group_ttest(pb_df)\n",
        "res.head(50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fBlGa96a9I1U",
      "metadata": {
        "id": "fBlGa96a9I1U"
      },
      "outputs": [],
      "source": [
        "# prompt: Using dataframe res: volcano plot\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "# Create a new column for the negative log10 of the p-value\n",
        "res['-log10(pval)'] = -1 * res['pval'].apply('log10')\n",
        "\n",
        "# Create the volcano plot\n",
        "chart = alt.Chart(res).mark_point().encode(\n",
        "    # Map fold change to the x-axis\n",
        "    x='fold_change',\n",
        "    # Map the negative log10 of the p-value to the y-axis\n",
        "    y='-log10(pval)',\n",
        "    tooltip=['gene', 'fold_change', 'pval']\n",
        ").properties(\n",
        "    title='Volcano Plot'\n",
        ").interactive()\n",
        "\n",
        "# Display the chart\n",
        "chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05Zf4MCm9FWW",
      "metadata": {
        "id": "05Zf4MCm9FWW"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_0['fold_change'].plot(kind='hist', bins=20, title='fold_change')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9PF8QDD9CP6",
      "metadata": {
        "id": "D9PF8QDD9CP6"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_0['fold_change'].plot(kind='hist', bins=20, title='fold_change')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DrG394_q8930",
      "metadata": {
        "id": "DrG394_q8930"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_0['fold_change'].plot(kind='hist', bins=20, title='fold_change')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JEtvSbV86TOV",
      "metadata": {
        "id": "JEtvSbV86TOV"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KpAKC-4EaDSJ",
      "metadata": {
        "id": "KpAKC-4EaDSJ"
      },
      "outputs": [],
      "source": [
        "# Load and apply metadata\n",
        "meta = pd.read_csv('/content/drive/MyDrive/Documents/metadata.csv', index_col='sample_id')\n",
        "adata.obs = adata.obs.join(meta, on='sample_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n5EqDu2W618Y",
      "metadata": {
        "id": "n5EqDu2W618Y"
      },
      "outputs": [],
      "source": [
        "# Run your DGE function and get full DataFrame\n",
        "dge_df = DGE(\n",
        "    adata=adata,\n",
        "    groupby='classification',\n",
        "    target_group='Sarcoidosis',\n",
        "    reference_group='Control',\n",
        "    min_pct=0.1,\n",
        "    min_fold_change=1.1,\n",
        "    n_genes=None,\n",
        "    n_sample=10000,\n",
        "    return_df=True\n",
        ")\n",
        "\n",
        "# Adjust p-values and compute â€“log10\n",
        "dge_df['pval_adj'] = multipletests(dge_df['pval'], method='fdr_bh')[1]\n",
        "dge_df['neglog10_padj'] = -np.log10(dge_df['pval_adj'] + 1e-300)  # to avoid log(0)\n",
        "\n",
        "# Save to Google Drive\n",
        "output_path = '/content/drive/MyDrive/Documents/Sarcoidosis/dge_results.csv'  # adjust as needed\n",
        "dge_df.to_csv(output_path, index=False)\n",
        "print(f\"âœ… DGE results saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ml56kLe64Bh_",
      "metadata": {
        "id": "ml56kLe64Bh_"
      },
      "outputs": [],
      "source": [
        "# prompt: produce a bar plot with average values of \"IL2_res_score\" by 'Sample type' and  'CoVID-19 severity'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'adata' AnnData object is loaded and contains the required 'obs' columns\n",
        "# 'Sample type' and 'CoVID-19 severity' are in adata.obs\n",
        "# 'IL2_res_score' is also in adata.obs\n",
        "\n",
        "# Group by 'Sample type' and 'CoVID-19 severity' and calculate the mean of 'IL2_res_score'\n",
        "df_plot = adata.obs.groupby(['Sample type', 'CoVID-19 severity'])['IL2_res_score'].mean().reset_index()\n",
        "\n",
        "# Pivot the DataFrame for plotting, with 'Sample type' as index and 'CoVID-19 severity' as columns\n",
        "pivot_df = df_plot.pivot(index='Sample type', columns='CoVID-19 severity', values='IL2_res_score')\n",
        "\n",
        "# Plotting the bar plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "pivot_df.plot(kind='bar', ax=ax, colormap='viridis') # Using viridis colormap\n",
        "\n",
        "ax.set_ylabel('Average IL2_res_score')\n",
        "ax.set_xlabel('Sample Type')\n",
        "ax.set_title('Average IL2_res_score by Sample Type and CoVID-19 Severity')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='CoVID-19 Severity', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5socp4YXjZlR",
      "metadata": {
        "id": "5socp4YXjZlR"
      },
      "outputs": [],
      "source": [
        "adata=adata_full"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617f3447-06b7-4435-9cf0-1c11957debf2",
      "metadata": {
        "id": "617f3447-06b7-4435-9cf0-1c11957debf2"
      },
      "source": [
        "### split UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ef527d-2860-407b-9dc4-a7b0043854a8",
      "metadata": {
        "id": "b8ef527d-2860-407b-9dc4-a7b0043854a8"
      },
      "outputs": [],
      "source": [
        "# Define your treatment conditions (or infer from the data)\n",
        "conditions = adata_full.obs['treatment'].unique().tolist()\n",
        "\n",
        "# Count cells per condition and find the minimum\n",
        "counts = adata_full.obs['treatment'].value_counts()\n",
        "min_n = counts.min()\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, len(conditions), figsize=(6 * len(conditions), 5))\n",
        "gene = 'FastTag'\n",
        "rng = np.random.default_rng(seed=0)\n",
        "\n",
        "for cond, ax in zip(conditions, axes):\n",
        "    # Get all cell barcodes for this condition\n",
        "    cells = adata_full.obs_names[adata_full.obs['treatment'] == cond]\n",
        "    # Downsample if necessary\n",
        "    if len(cells) > min_n:\n",
        "        chosen = rng.choice(cells, size=min_n, replace=False)\n",
        "    else:\n",
        "        chosen = cells\n",
        "    # Subset AnnData and plot UMAP\n",
        "    ad = adata_full[chosen].copy()\n",
        "    sc.pl.umap(\n",
        "        ad,\n",
        "        color=gene,\n",
        "        ax=ax,\n",
        "        show=False,\n",
        "        size=5,\n",
        "        vmax=7,\n",
        "        legend_loc=None,\n",
        "        legend_fontsize=12,\n",
        "        legend_fontweight= 'regular',\n",
        "        palette = set4,\n",
        "        title=None\n",
        "    )\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title('')  # this removes the auto-generated 'FastTag' title\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adata"
      ],
      "metadata": {
        "id": "2g11oU3_TVCb"
      },
      "id": "2g11oU3_TVCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d18875fa-92ec-4265-84f3-9bb248eb5a9b",
      "metadata": {
        "id": "d18875fa-92ec-4265-84f3-9bb248eb5a9b"
      },
      "source": [
        "\n",
        "### ANOVA + POST-HOC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f0c379-65f9-48bb-a507-36c5c9fabaff",
      "metadata": {
        "id": "f7f0c379-65f9-48bb-a507-36c5c9fabaff"
      },
      "outputs": [],
      "source": [
        "table = analyze_cell_type_distribution(adata, groupby='disease',\n",
        "    control_group='normal', key_added='FastTag', sample = \"donor_id\", paired = False)\n",
        "table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U3S8fBsEvpgQ",
      "metadata": {
        "id": "U3S8fBsEvpgQ"
      },
      "source": [
        "### Volcano AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tSAVn5Nr0kIX",
      "metadata": {
        "id": "tSAVn5Nr0kIX"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for cell_type, ad in adata_dict.items():\n",
        "    try:\n",
        "        df = DGE(\n",
        "            adata=ad,\n",
        "            groupby='treatment',\n",
        "            target_group=['treated'],\n",
        "            reference_group='untreated',\n",
        "            direction = 'both',\n",
        "            min_pct=0.25,\n",
        "            min_log2fc=0.15,\n",
        "            n_genes=None,\n",
        "            n_sample=5000,\n",
        "            return_df=True\n",
        "        )\n",
        "        if df.empty:\n",
        "            print(f\"âš ï¸ {cell_type}: No genes passed filtering.\")\n",
        "            continue\n",
        "\n",
        "        auc_up, auc_down = volcano_auc_split(df)\n",
        "\n",
        "        results.append({\n",
        "            'FastTag': cell_type,\n",
        "            'AUC_up': auc_up,\n",
        "            'AUC_down': auc_down,\n",
        "            'n_genes_tested': df.shape[0]\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {cell_type}: Error â€“ {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Assuming 'results' is a list of dicts with keys: FastTag, AUC_up, AUC_down, n_genes_tested\n",
        "auc_table = pd.DataFrame(results).set_index('FastTag')\n",
        "\n",
        "# Create grouped bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "bar_width = 0.4\n",
        "x = np.arange(len(auc_table.index))\n",
        "\n",
        "# Bars\n",
        "bars1 = ax.bar(x - bar_width/2, auc_table['AUC_up'], bar_width, label='Upregulated AUC', color='#1f77b4')\n",
        "bars2 = ax.bar(x + bar_width/2, auc_table['AUC_down'], bar_width, label='Downregulated AUC', color='#d62728')\n",
        "\n",
        "# Labels and style\n",
        "ax.set_xlabel('Cell Type (FastTag)')\n",
        "ax.set_ylabel('Volcano AUC')\n",
        "ax.set_title('Volcano AUC by Cell Type (Up vs Down)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(auc_table.index, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2at6rJPDwRK8",
      "metadata": {
        "id": "2at6rJPDwRK8"
      },
      "source": [
        "### Timecourse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yNB1NYlVS2At",
      "metadata": {
        "id": "yNB1NYlVS2At"
      },
      "outputs": [],
      "source": [
        "sc.tl.score_genes(adata, gene_list=markers ('IL2_res'), score_name='IL2_score')\n",
        "# adata.obs['IL2_score_minmax'] = minmax_scale(adata.obs['IL2_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UX8IdB1dVjuq",
      "metadata": {
        "id": "UX8IdB1dVjuq"
      },
      "outputs": [],
      "source": [
        "adata.obs['COND_numeric'] = adata.obs['COND'].map({'H': 0, 'T1D': 1}).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TVPVzD4emK-9",
      "metadata": {
        "id": "TVPVzD4emK-9"
      },
      "outputs": [],
      "source": [
        "genes =['VIM']\n",
        "for gene in genes:\n",
        "  plot_timecourse(\n",
        "  adata = adata,\n",
        "  relative = False,\n",
        "  features=[gene], # features should be a list\n",
        "  patient_key=None,\n",
        "  time_key='COND_numeric',\n",
        "  split_by= None,\n",
        "  color_by = 'sample_id',\n",
        "  sem= True,\n",
        "  palette=  plt.cm.winter,\n",
        "  figsize=(5, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MbdhoUrZvFU1",
      "metadata": {
        "id": "MbdhoUrZvFU1"
      },
      "source": [
        "### Cell time course"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qN_dXjb-O9ih",
      "metadata": {
        "id": "qN_dXjb-O9ih"
      },
      "outputs": [],
      "source": [
        "# Convert 'timepoint' column to integers before calling the function\n",
        "# Extract the number part from the string and convert to integer\n",
        "adata.obs['timepoint_int'] = adata.obs['timepoint'].str.replace('d', '').astype(int)\n",
        "\n",
        "\n",
        "cell_time_course(\n",
        "    adata,\n",
        "    patient_key='patient',\n",
        "    time_key='week', # Use the new integer timepoint column\n",
        "    key_added='FastTag',\n",
        "    cell_label='Treg',\n",
        "    figsize=(8, 4),\n",
        "    split_by = \"trial\",\n",
        "    palette=  plt.cm.winter\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C5stqoW563sL",
      "metadata": {
        "id": "C5stqoW563sL"
      },
      "source": [
        "### Produce pseudobulk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xAKfcDJnMlu",
      "metadata": {
        "id": "1xAKfcDJnMlu"
      },
      "outputs": [],
      "source": [
        "pb_Treg = generate_pseudobulk_matrix(adata, groupby_cols=['patient', 'treatment'])\n",
        "\n",
        "\n",
        "def plot_pseudobulk_timecourse(pb_df, gene: str, time_col='treatment', subject_col='patient'):\n",
        "\n",
        "    df = pb_df[[subject_col, time_col, gene]].dropna()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    ax = sns.lineplot(data=df, x=time_col, y=gene, hue=subject_col, marker=\"o\")\n",
        "\n",
        "    # Move legend to the right\n",
        "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
        "    plt.title(f\"{gene} by patient over time\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_pseudobulk_timecourse(pb_Treg, gene='TOMM22')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FLanq_Mm2VfV",
      "metadata": {
        "id": "FLanq_Mm2VfV"
      },
      "outputs": [],
      "source": [
        "ttest_pseudobulk(pb_df=pb_Treg,gene=\"CISH\",alternative='neg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oirryipm40k7",
      "metadata": {
        "id": "Oirryipm40k7"
      },
      "outputs": [],
      "source": [
        "def scan_ttest_all_genes(\n",
        "    pb_df,\n",
        "    gene_list=None,\n",
        "    condition_col=\"treatment\",\n",
        "    subject_col=\"patient\",\n",
        "    paired=True,\n",
        "    alternative=\"neg\",\n",
        "    alpha=0.05,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run t-test for all genes in pseudobulk matrix and return sorted significant hits.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pb_df : pd.DataFrame\n",
        "        Pseudobulk matrix.\n",
        "    gene_list : list of str or None\n",
        "        List of gene columns to test. If None, infer from pb_df (exclude obs columns).\n",
        "    condition_col : str\n",
        "        Column for treatment/condition.\n",
        "    subject_col : str\n",
        "        Column for subject ID.\n",
        "    paired : bool\n",
        "        Use paired t-test.\n",
        "    alternative : str\n",
        "        'both', 'pos', or 'neg' for direction of test.\n",
        "    alpha : float\n",
        "        Significance threshold (default = 0.05).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame with columns: gene, p_value, t_stat, passed\n",
        "        Sorted by p-value ascending.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    obs_cols = {condition_col, subject_col}\n",
        "    if gene_list is None:\n",
        "        gene_list = [col for col in pb_df.columns if col not in obs_cols]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for gene in gene_list:\n",
        "        try:\n",
        "            t_stat, p_val, _ = ttest_pseudobulk(\n",
        "                pb_df, gene, condition_col, subject_col, paired, alternative\n",
        "            )\n",
        "            results.append({\n",
        "                \"gene\": gene,\n",
        "                \"p_value\": p_val,\n",
        "                \"t_stat\": t_stat,\n",
        "                \"passed\": p_val < alpha\n",
        "            })\n",
        "        except Exception as e:\n",
        "            # Skip genes that fail (e.g. due to too many NaNs)\n",
        "            continue\n",
        "\n",
        "    res_df = pd.DataFrame(results)\n",
        "    res_df = res_df[res_df[\"passed\"]].sort_values(\"p_value\").reset_index(drop=True)\n",
        "    return res_df\n",
        "\n",
        "res_df = scan_ttest_all_genes(\n",
        "    pb_df=pb_Treg,\n",
        "    alternative='neg'\n",
        ")\n",
        "\n",
        "res_df ['gene'].head(50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average IL2_res_score for each LIB, COND, and FastTag combination\n",
        "score_table = adata.obs.groupby(['sample'])['IL2_score'].mean().unstack()\n",
        "\n",
        "# Skip rows with missing values (samples with more than one COND)\n",
        "score_table_filtered = score_table.dropna()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(score_table_filtered, cmap='viridis', annot=True, fmt=\".2f\")\n",
        "plt.title('Average IL2_res_score by LIB, COND, and FastTag (Filtered)')\n",
        "plt.xlabel('FastTag')\n",
        "plt.ylabel('LIB and COND')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ulgws8wJLB5Z"
      },
      "id": "ulgws8wJLB5Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mA2eGX81aNyS",
      "metadata": {
        "id": "mA2eGX81aNyS"
      },
      "outputs": [],
      "source": [
        "res_df ['gene'].head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YFWEApkwAGS4",
      "metadata": {
        "id": "YFWEApkwAGS4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a mapping with \"00\" prefix and string keys\n",
        "patient_trial_map = {\n",
        "    \"002003\": \"TILT\", \"002004\": \"TILT\", \"002005\": \"TILT\", \"002008\": \"T1D\", \"002009\": \"TILT\",\n",
        "    \"002011\": \"TILT\", \"002015\": \"T1D\", \"002017\": \"T1D\", \"002018\": \"T1D\", \"002019\": \"T1D\",\n",
        "    \"002022\": \"T1D\", \"007001\": \"TILT\", \"007002\": \"TILT\", \"007003\": \"TILT\", \"007005\": \"TILT\",\n",
        "    \"007101\": \"T1D\", \"007102\": \"T1D\", \"007103\": \"T1D\"\n",
        "}\n",
        "\n",
        "# Convert patient IDs to strings with \"00\" prefix\n",
        "adata.obs['patient_str'] = '00' + adata.obs['patient'].astype(str)\n",
        "\n",
        "# Map to trial column\n",
        "\n",
        "adata.obs['trial'] = adata.obs['patient'].astype(str).map(patient_trial_map)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8B2f6xlZ3PC",
      "metadata": {
        "id": "d8B2f6xlZ3PC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "pb_Treg['label'] = (pb_Treg['treatment'] == 'treated').astype(int)\n",
        "\n",
        "# 1. Subset to preselected genes\n",
        "X = pb_Treg[genes]\n",
        "y = pb_Treg['label']  # make sure this is 0 (control) / 1 (IL-2 treated)\n",
        "\n",
        "# 2. Train/test split (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Standardize gene expression (important for L1 regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Fit L1-penalized logistic regression with cross-validation\n",
        "clf = LogisticRegressionCV(\n",
        "    penalty='l1',\n",
        "    solver='saga',\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    max_iter=10000,\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Predict on test set and compute AUC\n",
        "y_probs = clf.predict_proba(X_test_scaled)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "# 6. Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('IL-2 Exposure Classifier (Logistic Regression)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7. Extract selected genes (non-zero coefficients)\n",
        "coefs = pd.Series(clf.coef_[0], index=genes)\n",
        "selected_genes = coefs[coefs != 0].sort_values(key=abs, ascending=False)\n",
        "print(\"Selected genes and their coefficients:\\n\")\n",
        "print(selected_genes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqOKmQvNuT5E",
      "metadata": {
        "id": "oqOKmQvNuT5E"
      },
      "outputs": [],
      "source": [
        "adata.obs['week'] = adata.obs['timepoint'].str.lstrip('d').astype(int) // 7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatterplot"
      ],
      "metadata": {
        "id": "5j57Xfniu8Bv"
      },
      "id": "5j57Xfniu8Bv"
    },
    {
      "cell_type": "code",
      "source": [
        "adata.obs['group'].unique()"
      ],
      "metadata": {
        "id": "3w9y0xscGgi-"
      },
      "id": "3w9y0xscGgi-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43530ff9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Define order and colors\n",
        "treatment_order = ['H_untreated', 'T1D_untreated', 'T1D_treated']\n",
        "custom_palette = {'H_untreated': 'blue', 'T1D_untreated': 'darkcyan', 'T1D_treated': 'green'}\n",
        "\n",
        "# Compute per-sample average IL2 score\n",
        "avg_df = (\n",
        "    adata.obs.groupby(['group', 'sample'])['IL2_score']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "avg_df['group'] = pd.Categorical(avg_df['group'], categories=treatment_order, ordered=True)\n",
        "\n",
        "# Compute mean per group (for horizontal bars)\n",
        "mean_scores = avg_df.groupby('group', sort=False)['IL2_score'].mean()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(4, 6))\n",
        "ax = plt.gca()\n",
        "\n",
        "# Swarmplot (goes behind)\n",
        "sns.swarmplot(\n",
        "    data=avg_df,\n",
        "    x='group', y='IL2_score',\n",
        "    hue='group', palette=custom_palette,\n",
        "    order=treatment_order, hue_order=treatment_order,\n",
        "    size=5, ax=ax, zorder=1\n",
        ")\n",
        "\n",
        "# CI-only pointplot (on top)\n",
        "sns.pointplot(\n",
        "    data=avg_df,\n",
        "    x='group', y='IL2_score',\n",
        "    estimator='mean', errorbar='ci',\n",
        "    capsize=0.05, color='black',\n",
        "    errwidth=1, linewidth=0,\n",
        "    order=treatment_order, ax=ax, zorder=10\n",
        ")\n",
        "\n",
        "# Horizontal mean bars\n",
        "for i, group in enumerate(treatment_order):\n",
        "    ax.hlines(mean_scores[group], i - 0.2, i + 0.2, color='black', linewidth=1.5, zorder=11)\n",
        "\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Extract groups\n",
        "groups = [avg_df.loc[avg_df['group'] == g, 'IL2_score'] for g in treatment_order]\n",
        "\n",
        "# Run one-way ANOVA\n",
        "groups = [\n",
        "    avg_df.loc[avg_df['group'] == g, 'IL2_score'].dropna()\n",
        "    for g in treatment_order\n",
        "]\n",
        "f_stat, p_val = f_oneway(*groups)\n",
        "\n",
        "# Annotate p-value on plot\n",
        "ax.text(\n",
        "    0.5,                        # x-position (centered)\n",
        "    ax.get_ylim()[1] * 0.95,    # y-position (top 5%)\n",
        "    f\"ANOVA p = {p_val:.4e}\",   # formatted p-value\n",
        "    ha='center', fontsize=10\n",
        ")\n",
        "\n",
        "# Styling\n",
        "ax.set_title('Average IL2_score per Sample ID by group with Mean and 95% CI')\n",
        "ax.set_xlabel('group')\n",
        "ax.set_ylabel('Average IL2_score per Sample')\n",
        "ax.set_xticks(range(len(treatment_order)))\n",
        "ax.set_xticklabels(treatment_order)\n",
        "ax.legend([], [], frameon=False)  # Remove redundant hue legend\n",
        "\n",
        "# Overlay: different markers for dataset_origin\n",
        "avg_df['x_numeric'] = avg_df['group'].cat.codes  # get numeric x positions\n",
        "sns.scatterplot(\n",
        "    data=avg_df,\n",
        "    x='x_numeric', y='IL2_score',\n",
        "    style=adata.obs.loc[avg_df['sample'], 'dataset_origin'].values,\n",
        "    markers={'A': 'o', 'H': 's'}, s=50,\n",
        "    hue='group', palette=custom_palette,\n",
        "    ax=ax, legend=False, zorder=2\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "43530ff9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group display order and colors\n",
        "group_order = ['H_untreated', 'T1D_untreated', 'T1D_treated']\n",
        "color_map = {'H_untreated': 'blue', 'T1D_untreated': 'darkcyan', 'T1D_treated': 'green'}\n",
        "marker_map = {'H': 's', 'A': 'o'}  # H = Healthy, A = Autoimmune\n",
        "\n",
        "# Compute per-sample averages\n",
        "avg_df = (\n",
        "    adata.obs.groupby(['group', 'sample', 'dataset_origin'])['IL2_score']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "avg_df['group'] = pd.Categorical(avg_df['group'], categories=group_order, ordered=True)\n",
        "\n",
        "# Compute group means and 95% CIs\n",
        "summary_stats = avg_df.groupby('group')['IL2_score'].agg(['mean', 'count', 'std'])\n",
        "summary_stats['ci95'] = 1.96 * summary_stats['std'] / np.sqrt(summary_stats['count'])\n",
        "\n",
        "# Plot setup\n",
        "fig, ax = plt.subplots(figsize=(4, 6))\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "# Scatter each point with color + marker by group and origin\n",
        "for i, group in enumerate(group_order):\n",
        "    sub = avg_df[avg_df['group'] == group]\n",
        "    jittered_x = rng.normal(loc=i, scale=0.08, size=len(sub))\n",
        "    for x, y, origin in zip(jittered_x, sub['IL2_score'], sub['dataset_origin']):\n",
        "        ax.scatter(\n",
        "            x, y,\n",
        "            color=color_map[group],\n",
        "            marker=marker_map.get(origin, 'o'),\n",
        "            edgecolor='black',\n",
        "            linewidth=0.4,\n",
        "            s=40,\n",
        "            alpha=0.85\n",
        "        )\n",
        "\n",
        "# Add CI bars and mean lines\n",
        "for i, group in enumerate(group_order):\n",
        "    mean = summary_stats.loc[group, 'mean']\n",
        "    ci = summary_stats.loc[group, 'ci95']\n",
        "    ax.errorbar(\n",
        "        i, mean, yerr=ci,\n",
        "        fmt='none', ecolor='black', elinewidth=1.2, capsize=4, zorder=10\n",
        "    )\n",
        "    ax.hlines(mean, i - 0.2, i + 0.2, color='black', linewidth=1.5, zorder=11)\n",
        "\n",
        "# Optional legend for marker shape\n",
        "import matplotlib.lines as mlines\n",
        "legend_elements = [\n",
        "    mlines.Line2D([], [], color='black', marker='s', linestyle='None', label='Healthy dataset', markersize=6),\n",
        "    mlines.Line2D([], [], color='black', marker='o', linestyle='None', label='Autoimmune dataset', markersize=6),\n",
        "]\n",
        "ax.legend(handles=legend_elements, title='Origin', loc='upper left')\n",
        "\n",
        "# Styling\n",
        "ax.set_title('Average IL2_score per Sample by Group and Origin')\n",
        "ax.set_xticks(range(len(group_order)))\n",
        "ax.set_xticklabels(group_order)\n",
        "ax.set_ylabel('IL2_score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6MphIz5ZF4wz"
      },
      "id": "6MphIz5ZF4wz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 1: Score genes and split ===\n",
        "\n",
        "if True:\n",
        "    gene_set_names = [\n",
        "        'e_Treg',\n",
        "        'IL2_res'\n",
        "    ]\n",
        "    score_names = [name + \"_score\" for name in gene_set_names]\n",
        "\n",
        "    for gene_set in gene_set_names:\n",
        "        sc.tl.score_genes(adata, markers(gene_set), score_name=gene_set + \"_score\")\n",
        "\n",
        "    adata_dict = split(adata, category='FastTag', min_cells=30)\n",
        "\n",
        "# === Step 2: Aggregate per-donor scores for each cell type ===\n",
        "\n",
        "cell_type_1 = \"Treg\"\n",
        "cell_type_2 = \"Treg\"\n",
        "\n",
        "donor_scores = {}\n",
        "\n",
        "for cell_type in [cell_type_1, cell_type_2]:\n",
        "    ad = adata_dict.get(cell_type)\n",
        "    if ad is None:\n",
        "        raise ValueError(f\"No data found for cell type {cell_type}\")\n",
        "\n",
        "    df = (\n",
        "        ad.obs\n",
        "        .groupby(\"donor_id\", observed=True)[score_names]\n",
        "        .mean()\n",
        "        .add_prefix(f\"{cell_type}_\")\n",
        "    )\n",
        "    donor_scores[cell_type] = df\n",
        "\n",
        "# Merge dataframes on donor_id\n",
        "df_scores = pd.concat(donor_scores.values(), axis=1, join=\"inner\").reset_index()\n",
        "\n",
        "# === Step 3: Correlation analysis ===\n",
        "\n",
        "score_1 = f\"{cell_type_1}_IL2_res_score\"\n",
        "score_2 = f\"{cell_type_2}_e_Treg_score\"\n",
        "\n",
        "# Ensure both columns exist\n",
        "for col in [score_1, score_2]:\n",
        "    if col not in df_scores.columns:\n",
        "        raise KeyError(f\"Missing column: {col}\")\n",
        "\n",
        "# Pearson correlation\n",
        "r, p = pearsonr(df_scores[score_1], df_scores[score_2])\n",
        "print(f\"Pearson r = {r:.2f}, p = {p:.3g}\")\n",
        "\n",
        "# === Step 4: Plot ===\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.regplot(\n",
        "    data=df_scores,\n",
        "    x=score_1,\n",
        "    y=score_2,\n",
        "    scatter_kws={'s': 60},\n",
        "    line_kws={'color': 'gray'}\n",
        ")\n",
        "plt.xlabel(score_1)\n",
        "plt.ylabel(score_2)\n",
        "plt.title(f\"Per-donor correlation\\nr = {r:.2f}, p = {p:.3g}\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pPSjGbarvSCa"
      },
      "id": "pPSjGbarvSCa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ac72114b-0f3a-4e2c-a665-409b998c8afb",
      "metadata": {
        "id": "ac72114b-0f3a-4e2c-a665-409b998c8afb"
      },
      "source": [
        "## Save files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4c7f04-8418-419d-aa82-4ae938dc099e",
      "metadata": {
        "id": "1c4c7f04-8418-419d-aa82-4ae938dc099e"
      },
      "outputs": [],
      "source": [
        "# to Drive\n",
        "adata.write ('Dong/Dong_T.h5ad') # save file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MbLS3iD6YaiJ",
      "metadata": {
        "id": "MbLS3iD6YaiJ"
      },
      "outputs": [],
      "source": [
        "# to google cloud\n",
        "\n",
        "adata.write (\"/content/FL_T.h5ad\")\n",
        "blob = bucket.blob(\"FL/FL_T.h5ad\")\n",
        "blob.upload_from_filename(\"/content/FL_T.h5ad\")\n",
        "blob = bucket.blob(\"markers.yaml\"); blob.upload_from_filename(\"markers.yaml\")\n",
        "blob = bucket.blob(\"ontology.yaml\"); blob.upload_from_filename(\"ontology.yaml\")\n",
        "blob = bucket.blob(\"ontology_TIL.yaml\"); blob.upload_from_filename(\"ontology_TIL.yaml\")\n",
        "blob = bucket.blob(\"coarse.yaml\"); blob.upload_from_filename(\"coarse.yaml\")\n",
        "blob = bucket.blob(\"notes.txt\"); blob.upload_from_filename(\"notes.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F04i3_99stxC"
      },
      "id": "F04i3_99stxC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hZUZHOog3Rwj",
        "veAlkrPm24F4",
        "8eba92a2-32d9-4b4b-925f-85be45af65f4"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}